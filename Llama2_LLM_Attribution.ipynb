{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9bdd6ea2",
   "metadata": {},
   "source": [
    "# Understanding Llama2 with Captum LLM Attribution\n",
    "\n",
    "In this tutorial, we will demonstrate the LLM attribution functionality introduced in Captum v0.7, which makes it a breeze to applying the attribution algorithms to interpret the large langague models (LLM) in text generation. The new functionalities include a series utilities that help you with many common tedious scaffolding required by LLMs like defining intepretable features in text input and handling the sequential predictions. You can also check our paper for more details https://arxiv.org/abs/2312.05491\n",
    "\n",
    "Next, we will use Llama2 (7b-chat) as an example and use both perturbation-based and grandient-based algrithms respectively to see how the input prompts lead to the generated content. First, let's import the needed dependencies. Specifically, from Captum, besides the algorithms `FeatureAblation` and `LayerIntegratedGradients` themselves, we will also import:\n",
    "- perturbation-based and gradient-based wrappers for LLM, `LLMAttribution` and `LLMGradientAttribution`\n",
    "- text-based interpretable input adapters, `TextTokenInput` and `TextTemplateInput`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "inside-current",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n"
     ]
    }
   ],
   "source": [
    "#https://captum.ai/tutorials/Llama2_LLM_Attribution\n",
    "\n",
    "#install directly from https://github.com/pytorch/captum/tree/master\n",
    "import warnings\n",
    "\n",
    "import bitsandbytes as bnb\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "\n",
    "from captum.attr import (\n",
    "    FeatureAblation, \n",
    "    ShapleyValues,\n",
    "    LayerIntegratedGradients, \n",
    "    LLMAttribution, \n",
    "    LLMGradientAttribution, \n",
    "    TextTokenInput, \n",
    "    TextTemplateInput,\n",
    "    ProductBaselines,\n",
    "    LayerIntegratedGradients,\n",
    ")\n",
    "\n",
    "# Ignore warnings due to transformers library\n",
    "warnings.filterwarnings(\"ignore\", \".*past_key_values.*\")\n",
    "warnings.filterwarnings(\"ignore\", \".*Skipping this token.*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8b51ee9f-9373-4c57-89a1-aba4f45d6824",
   "metadata": {},
   "outputs": [],
   "source": [
    "from captum.attr._utils.interpretable_input import InterpretableInput\n",
    "from typing import Union,Optional,Dict,cast\n",
    "from torch import Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e373d480-4a26-47fc-85be-0259f85b9566",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LLMGradientAttribution_Features(LLMGradientAttribution):\n",
    "    def attribute(\n",
    "        self,\n",
    "        inp: InterpretableInput,\n",
    "        target: Union[str, torch.Tensor, None] = None,\n",
    "        gen_args: Optional[Dict] = None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            inp (InterpretableInput): input prompt for which attributions are computed\n",
    "            target (str or Tensor, optional): target response with respect to\n",
    "                    which attributions are computed. If None, it uses the model\n",
    "                    to generate the target based on the input and gen_args.\n",
    "                    Default: None\n",
    "            gen_args (dict, optional): arguments for generating the target. Only used if\n",
    "                    target is not given. When None, the default arguments are used,\n",
    "                    {\"max_length\": 25, \"do_sample\": False}\n",
    "                    Defaults: None\n",
    "            **kwargs (Any): any extra keyword arguments passed to the call of the\n",
    "                    underlying attribute function of the given attribution instance\n",
    "    \n",
    "        Returns:\n",
    "    \n",
    "            attr (LLMAttributionResult): attribution result\n",
    "        \"\"\"\n",
    "    \n",
    "        assert isinstance(\n",
    "            inp, self.SUPPORTED_INPUTS\n",
    "        ), f\"LLMGradAttribution does not support input type {type(inp)}\"\n",
    "    \n",
    "        if target is None:\n",
    "            # generate when None\n",
    "            assert hasattr(self.model, \"generate\") and callable(self.model.generate), (\n",
    "                \"The model does not have recognizable generate function.\"\n",
    "                \"Target must be given for attribution\"\n",
    "            )\n",
    "    \n",
    "            if not gen_args:\n",
    "                gen_args = DEFAULT_GEN_ARGS\n",
    "    \n",
    "            model_inp = self._format_model_input(inp.to_model_input())\n",
    "            output_tokens = self.model.generate(model_inp, **gen_args)\n",
    "            target_tokens = output_tokens[0][model_inp.size(1) :]\n",
    "        else:\n",
    "            assert gen_args is None, \"gen_args must be None when target is given\"\n",
    "    \n",
    "            if type(target) is str:\n",
    "                # exclude sos\n",
    "                target_tokens = self.tokenizer.encode(target)[1:]\n",
    "                target_tokens = torch.tensor(target_tokens)\n",
    "            elif type(target) is torch.Tensor:\n",
    "                target_tokens = target\n",
    "    \n",
    "        attr_inp = inp.to_tensor().to(self.device)\n",
    "    \n",
    "        attr_list = []\n",
    "        for cur_target_idx, _ in enumerate(target_tokens):\n",
    "            # attr in shape(batch_size, input+output_len, emb_dim)\n",
    "            attr = self.attr_method.attribute(\n",
    "                attr_inp,\n",
    "                additional_forward_args=(\n",
    "                    inp,\n",
    "                    target_tokens,\n",
    "                    cur_target_idx,\n",
    "                ),\n",
    "                **kwargs,\n",
    "            )\n",
    "            attr = cast(Tensor, attr)\n",
    "    \n",
    "            # will have the attr for previous output tokens\n",
    "            # cut to shape(batch_size, inp_len, emb_dim)\n",
    "            if cur_target_idx:\n",
    "                attr = attr[:, :-cur_target_idx]\n",
    "    \n",
    "            # the author of IG uses sum\n",
    "            # https://github.com/ankurtaly/Integrated-Gradients/blob/master/BertModel/bert_model_utils.py#L350\n",
    "            #print(attr.shape)\n",
    "            #attr = attr.sum(-1)\n",
    "            attr_list.append(attr)\n",
    "    \n",
    "        # assume inp batch only has one instance\n",
    "        # to shape(n_output_token, ...)\n",
    "        \n",
    "    \n",
    "        return attr_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "15f4b3f3-ccac-40b6-b3b5-d6a4f2c633e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\denis\\\\captum\\\\captum\\\\__init__.py'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import captum\n",
    "captum.__file__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f2695ee",
   "metadata": {},
   "source": [
    "## Preparation\n",
    "\n",
    "Let's make a helper function to load models through Huggingface. We will also add an extra step for 4-bits quantization which can effectively reduce the GPU memory consumption. Now, we can use them to load \"Llama-2-7b-chat\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "driven-privacy",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_name, bnb_config):\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, token=True)\n",
    "\n",
    "    # Needed for LLaMA tokenizer\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    return model, tokenizer\n",
    "\n",
    "def create_bnb_config():\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    )\n",
    "\n",
    "    return bnb_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "exclusive-ministry",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"meta-llama/Llama-3.2-1B\" \n",
    "\n",
    "bnb_config = create_bnb_config()\n",
    "\n",
    "model, tokenizer = load_model(model_name, bnb_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "12a60b96-5bb9-424f-999c-6093e4df47b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "layers_to_hook_layer_list=[]\n",
    "layers_to_hook_name_list=[]\n",
    "\n",
    "layers_to_hook = {}\n",
    "layers_to_hook[\"embed_tokens\"]=[model.model.embed_tokens]\n",
    "layers_to_hook_layer_list.append(model.model.embed_tokens)\n",
    "layers_to_hook_name_list.append([\"embed_tokens\",0])\n",
    "\n",
    "for i_p,i in enumerate(model.model.layers[:-1]):\n",
    "    \n",
    "    if \"q_proj\" not in layers_to_hook:\n",
    "        layers_to_hook[\"q_proj\"]=[]\n",
    "    layers_to_hook[\"q_proj\"].append(i.self_attn.q_proj)\n",
    "    layers_to_hook_layer_list.append(i.self_attn.q_proj)\n",
    "    layers_to_hook_name_list.append([\"q_proj\",i_p])\n",
    "\n",
    "    if \"k_proj\" not in layers_to_hook:\n",
    "        layers_to_hook[\"k_proj\"]=[]\n",
    "    layers_to_hook[\"k_proj\"].append(i.self_attn.k_proj)\n",
    "    layers_to_hook_layer_list.append(i.self_attn.k_proj)\n",
    "    layers_to_hook_name_list.append([\"k_proj\",i_p])\n",
    "\n",
    "    if \"v_proj\" not in layers_to_hook:\n",
    "        layers_to_hook[\"v_proj\"]=[]\n",
    "    layers_to_hook[\"v_proj\"].append(i.self_attn.v_proj)\n",
    "    layers_to_hook_layer_list.append(i.self_attn.v_proj)\n",
    "    layers_to_hook_name_list.append([\"v_proj\",i_p])\n",
    "    \"\"\"\n",
    "    if \"o_proj\" not in layers_to_hook:\n",
    "        layers_to_hook[\"o_proj\"]=[]\n",
    "    layers_to_hook[\"o_proj\"].append(i.self_attn.o_proj)\n",
    "    layers_to_hook_layer_list.append(i.self_attn.o_proj)\n",
    "    layers_to_hook_name_list.append([\"o_proj\",i_p])\n",
    "    \"\"\"\n",
    "\n",
    "    #Is not used in this setting (no gradients found)\n",
    "    #if \"rotary_emb\" not in layers_to_hook:\n",
    "    #    layers_to_hook[\"rotary_emb\"]=[]\n",
    "    #layers_to_hook[\"rotary_emb\"].append(i.self_attn.rotary_emb)\n",
    "    \n",
    "    if \"mlp\" not in layers_to_hook:\n",
    "        layers_to_hook[\"mlp\"]=[]\n",
    "    layers_to_hook[\"mlp\"].append(i.mlp)\n",
    "    layers_to_hook_layer_list.append(i.mlp)\n",
    "    layers_to_hook_name_list.append([\"mlp\",i_p])\n",
    "\n",
    "layers_to_hook[\"rotary_emb_end\"]=[model.model.rotary_emb]\n",
    "layers_to_hook_layer_list.append(model.model.rotary_emb)\n",
    "layers_to_hook_name_list.append([\"rotary_emb_end\",0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "egyptian-uncertainty",
   "metadata": {},
   "source": [
    "Let's test the model with a simple prompt and take a look at the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "suffering-reconstruction",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dave lives in Palm Coast, FL and is a lawyer. His personal interests include fishing, gardening, and travel. He enjoys helping people and is a member\n"
     ]
    }
   ],
   "source": [
    "eval_prompt = \"Dave lives in Palm Coast, FL and is a lawyer. His personal interests include\"\n",
    "\n",
    "model_input = tokenizer(eval_prompt, return_tensors=\"pt\")\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    output_ids = model.generate(model_input[\"input_ids\"], max_new_tokens=15)[0]\n",
    "    response = tokenizer.decode(output_ids, skip_special_tokens=True)\n",
    "    print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "urban-insight",
   "metadata": {},
   "source": [
    "## Perturbation-based Attribution\n",
    "\n",
    "OK now, the model is working and has completed the given prompt by producing several possible interests. To understand how the model produces them based on the prompt, we will first use the perturbation-based algrotihms from Captum to understand the generation. We can start with the simplest `FeatureAblation`, which ablates each of the features of this string to see how it affects the predicted probability of the target string. The way is the same as before: feed the model into the corresponding constructor to initiate the attribution method. But additionally, to help it work with text-based input and output, we need to wrap it with the new `LLMAttribution` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0d30d6a6-2309-4849-aef3-9af800b7b676",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(128256, 2048)\n",
       "    (layers): ModuleList(\n",
       "      (0-15): 16 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaSdpaAttention(\n",
       "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (k_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "          (v_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "          (up_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "          (down_proj): Linear(in_features=8192, out_features=2048, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2048, out_features=128256, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "hairy-seeking",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\denis\\AppData\\Local\\Temp\\ipykernel_10708\\1377343257.py:1: UserWarning: Multiple layers provided. Please ensure that each layer is**not** solely dependent on the outputs ofanother layer. Please refer to the documentation for moredetail.\n",
      "  fa = LayerIntegratedGradients(model,layers_to_hook_layer_list)\n"
     ]
    }
   ],
   "source": [
    "fa = LayerIntegratedGradients(model,layers_to_hook_layer_list)\n",
    "\n",
    "llm_attr = LLMGradientAttribution_Features(fa, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "494d2a02",
   "metadata": {},
   "source": [
    "The newly created `llm_attr` is the same as the wrapped attribution method instance which provides an `.attribute()` function taking the model inputs and returns the attribution scores of cared features within the inputs. However, by default, Captum's attribution algorithms assume each input into the model must be PyTorch tensors and perturb them at tensor level. This is likely not what we want for LLM, where we are more interested in the interpretable text input and making text modifications like removing a text segment, than a less meaningful tensor of token indices. To solve this, we introduce a new adapter design called `InterpretableInput` which handles the conversion between more interpretable input type and tensor, and tells Captum how to work with them. `llm_attr` is made to accept certain text-based `InterpretableInput` as the arguements. The concept of \"Interpretable Input\" mainly comes from the following two papers:\n",
    "- [“Why Should I Trust You?”: Explaining the Predictions of Any Classifier](https://arxiv.org/abs/1602.04938)\n",
    "- [A Unified Approach to Interpreting Model Predictions](https://arxiv.org/abs/1705.07874)\n",
    "\n",
    "The question now is what are the intepretable features we want to understand in text. One most common and straightforward answer is \"tokens\". And we provide `TextTokenInput` specifically for such use cases. `TextTokenInput` is an `InterpretableInput` for text whose interpretable features are the tokens with respect to a given tokenizer. So let's create one and calculate its attribution w.r.t the previous generated output as the target:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5be9d42e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "  # skip the special token for the start of the text <s>\n",
    "inp = TextTokenInput(\n",
    "    eval_prompt, \n",
    "    tokenizer\n",
    ")\n",
    "\n",
    "target = \"100\" #\"playing guitar, hiking, and spending time with his family.\"\n",
    "\n",
    "attr_res = llm_attr.attribute(inp, target=target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6c064c1a-7874-40f5-ad04-2c9ffe85c71d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<captum.attr._utils.interpretable_input.TextTokenInput at 0x1ba3eabb650>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a6fcc263-e323-4643-95ba-d8a343d5372e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 17, 2048])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attr_res[0][0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53921fcb",
   "metadata": {},
   "source": [
    "With just a few lines of codes, we now get the `FeatureAblation` attribution result of our LLM. The return contains the attribution tensors to both the entire generated target seqeuence and each generated token, which tell us how each input token impact the output and each token within it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dc68909e",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'seq_attr'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattr to the output sequence:\u001b[39m\u001b[38;5;124m\"\u001b[39m, attr_res\u001b[38;5;241m.\u001b[39mseq_attr\u001b[38;5;241m.\u001b[39mshape)  \u001b[38;5;66;03m# shape(n_input_token)\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattr to the output tokens:\u001b[39m\u001b[38;5;124m\"\u001b[39m, attr_res\u001b[38;5;241m.\u001b[39mtoken_attr\u001b[38;5;241m.\u001b[39mshape)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'seq_attr'"
     ]
    }
   ],
   "source": [
    "print(\"attr to the output sequence:\", attr_res.seq_attr.shape)  # shape(n_input_token)\n",
    "print(\"attr to the output tokens:\", attr_res.token_attr.shape)  # shape(n_output_token, n_input_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eacfb8f1",
   "metadata": {},
   "source": [
    "It also provides the utilities to visualize the results. Next we will plot the token attribution to view the relations between input and output tokens. As we will see, the result is generally very positive. This is expected, since the target, \"playing guitar, hiking, and spending time with his family\", is what the model feel confident to generate by itself given the input tokens. So change in the input is more likely divert the model from this target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aebdd52",
   "metadata": {},
   "outputs": [],
   "source": [
    "attr_res.plot_token_attr(show=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f039697",
   "metadata": {},
   "source": [
    "However, it may not always make sense to define individual token as intepretable features and perturb it. Tokenizers used in modern LLMs may break a single word making the tokens not intepretable by themselves. For example, in our case above, the tokenizer can break the word \"Palm\" into \"_Pal\" and \"m\". It doesn't make much sense to study the separate attribution of them. Moreover, even a whole word can be meaningless. For example, \"Palm Coast\" together result in a city name. Changing just partial of its tokens would likely not give anything belongs to the natural distribution of potential cities in Florida, which may lead to unexpected impacts on the perturbed model output.\n",
    "\n",
    "Therefore, Captum offers another more customizable interpretable input class, `TextTemplateInput`, whose interpretable features are certain segments (e.g., words, phrases) of the text defined by the users. For instance, our prompt above contains information about name, city, state, occupation, and pronoun. Let's define them as the interpretable features to get their attribution. \n",
    "\n",
    "The target to interpret can be any potential generations that we are interested in. Next, we will customize the target to something else."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0673a936",
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = TextTemplateInput(\n",
    "    template=\"{} lives in {}, {} and is a {}. {} personal interests include\", \n",
    "    values=[\"Dave\", \"Palm Coast\", \"FL\", \"lawyer\", \"His\"],\n",
    ")\n",
    " \n",
    "\n",
    "attr_res = llm_attr.attribute(inp, target=target, skip_tokens=skip_tokens)\n",
    "\n",
    "attr_res.plot_token_attr(show=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56535322",
   "metadata": {},
   "source": [
    "We know that perturbation-based algrotihms calculate the attribution by switching the features between \"presence\" and \"absence\" states. So what should a text feature look like here when it is in \"absence\" in the above example? Captum allows users to set the baselines, i.e., the reference values, to use when a feature is absent. By default, `TextTemplateInput` uses empty string `''` as the baselines for all, which is equivalent to the removal of the segments. This may not be perfect for the same out-of-distribution reason. For example, when the feature \"name\" is absent, the prompt loses its subjective and no longer makes much sense. \n",
    "\n",
    "To improve it, let's manually set the baselines to something that still fit the context of the original text and keep it within the natural data distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lined-eating",
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = TextTemplateInput(\n",
    "    template=\"{} lives in {}, {} and is a {}. {} personal interests include\", \n",
    "    values=[\"Dave\", \"Palm Coast\", \"FL\", \"lawyer\", \"His\"],\n",
    "    baselines=[\"Sarah\", \"Seattle\", \"WA\", \"doctor\", \"Her\"],\n",
    ")\n",
    "\n",
    "attr_res = llm_attr.attribute(inp, target=target, skip_tokens=skip_tokens)\n",
    "\n",
    "attr_res.plot_token_attr(show=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c34f5712",
   "metadata": {},
   "source": [
    "The result represents how the features impacts the output compared with the single baseline. It can be a useful setup to have some interesting findings. For example, the city name \"Palm Coast\" is more positive to \"playing golf\" but negative to \"hiking\" compared with \"Seattle\".\n",
    "\n",
    "But more generally, we would prefer a distribution of baselines so the attribution method will sample from for generosity. Here, we can leverage the `ProductBaselines` to define a Cartesian product of different baselines values of various features. And we can specify `num_trials` in attribute to average over multiple trials\n",
    "\n",
    "Another issue we notice from the above results is that there are correlated aspects of the prompt which should be ablated together to ensure that the input remain in distribution, e.g. Palm Coast, FL should be ablated with Seattle, WA. We can accomplish this using a mask as defined below, which will group (city, state) and (name, pronoun). `TextTemplateFeature` accepts the argument `mask` allowing us to set the group indices. To make it more explicit, we can also define the template and its values in dictionary format instead of list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "breathing-sound",
   "metadata": {},
   "outputs": [],
   "source": [
    "baselines = ProductBaselines(\n",
    "    {\n",
    "        (\"name\", \"pronoun\"):[(\"Sarah\", \"her\"), (\"John\", \"His\"), (\"Martin\", \"His\"), (\"Rachel\", \"Her\")],\n",
    "        (\"city\", \"state\"): [(\"Seattle\", \"WA\"), (\"Boston\", \"MA\")],\n",
    "        \"occupation\": [\"doctor\", \"engineer\", \"teacher\", \"technician\", \"plumber\"], \n",
    "    }\n",
    ")\n",
    "\n",
    "inp = TextTemplateInput(\n",
    "    \"{name} lives in {city}, {state} and is a {occupation}. {pronoun} personal interests include\", \n",
    "    values={\"name\": \"Dave\", \"city\": \"Palm Coast\", \"state\": \"FL\", \"occupation\": \"lawyer\", \"pronoun\": \"His\"}, \n",
    "    baselines=baselines,\n",
    "    mask={\"name\": 0, \"city\": 1, \"state\": 1, \"occupation\": 2, \"pronoun\": 0},\n",
    ")\n",
    "\n",
    "attr_res = llm_attr.attribute(inp, target=target, skip_tokens=skip_tokens, num_trials=3)\n",
    "\n",
    "attr_res.plot_token_attr(show=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "documented-harvard",
   "metadata": {},
   "source": [
    "One potential issue with the current approach is using Feature Ablation. If the model learns complex interations between the prompt features, the true importance may not be reflected in the attribution scores. Consider a case where the model predicts a high probability of playing golf if a person is either a lawyer or lives in Palm Coast. By ablating a feature one at a time, the probability may appear to be unchanged when ablating each feature independently, but may drop substantially when perturbing both together.\n",
    "\n",
    "To address this, we can apply alternate perturbation-based attribution methods available in Captum such as ShapleyValue(Sampling), KernelShap and Lime, which ablate different subgroups of features and may result in more accurate scores.\n",
    "\n",
    "We will use `ShapleyValue` below because we essentially only have three features now after grouping. The computation is tractable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "iraqi-gibson",
   "metadata": {},
   "outputs": [],
   "source": [
    "sv = ShapleyValues(model) \n",
    "\n",
    "sv_llm_attr = LLMAttribution(sv, tokenizer)\n",
    "\n",
    "attr_res = sv_llm_attr.attribute(inp, target=target, skip_tokens=skip_tokens, num_trials=3)\n",
    "\n",
    "attr_res.plot_token_attr(show=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "objective-america",
   "metadata": {},
   "source": [
    "Let's now consider a more complex example, where we use the LLM as a few-shot learner to classify sample movie reviews as positive or negative. We want to measure the relative impact of the few shot examples. Since the prompt changes slightly in the case that no examples are needed, we define a prompt function rather than a format string in this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "powered-seating",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompt_fn(*examples):\n",
    "    main_prompt = \"Decide if the following movie review enclosed in quotes is Positive or Negative:\\n'I really liked the Avengers, it had a captivating plot!'\\nReply only Positive or Negative.\"\n",
    "    subset = [elem for elem in examples if elem]\n",
    "    if not subset:\n",
    "        prompt = main_prompt\n",
    "    else:\n",
    "        prefix = \"Here are some examples of movie reviews and classification of whether they were Positive or Negative:\\n\"\n",
    "        prompt = prefix + \" \\n\".join(subset) + \"\\n \" + main_prompt\n",
    "    return \"[INST] \" + prompt + \"[/INST]\"\n",
    "\n",
    "input_examples = [\n",
    "    \"'The movie was ok, the actors weren't great' Negative\", \n",
    "    \"'I loved it, it was an amazing story!' Positive\",\n",
    "    \"'Total waste of time!!' Negative\", \n",
    "    \"'Won't recommend' Negative\",\n",
    "]\n",
    "inp = TextTemplateInput(\n",
    "    prompt_fn, \n",
    "    values=input_examples,\n",
    ")\n",
    "\n",
    "attr_res = sv_llm_attr.attribute(inp, skip_tokens=skip_tokens)\n",
    "\n",
    "attr_res.plot_token_attr(show=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa2739bf",
   "metadata": {},
   "source": [
    "Interestingly, we can see all these few-shot examples we choose actually make the model less likely to correctly label the given review as \"Positive\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c715ba4c-bd02-4e32-a9a8-f531187d5e3e",
   "metadata": {},
   "source": [
    "# Gradient-based Attribution\n",
    "As an alternative to perturbation-based attribution, we can use gradient-based methods to attribute each feature's contribution to a target sequence being generated. For LLMs, the only supported method at present is `LayerIntegratedGradients`. Layer Integrated Gradients is a variant of Integrated Gradients that assigns an importance score to layer inputs or outputs. Integrated Gradients works by assigning an importance score to each input feature by approximating the integral of gradients of a function's output with respect to the inputs along the path from given references to inputs. To instantiate, we can simply wrap our gradient-based attribution method with `LLMGradientAttribution`. Here, we measure the importance of each input token to the embedding layer `model.embed_tokens` of the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf080c0a-9c51-4c1b-8ca6-a01da213a4fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "lig = LayerIntegratedGradients(model, model.model.embed_tokens)\n",
    "\n",
    "llm_attr = LLMGradientAttribution(lig, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9f383cd-1246-4695-a96c-a0a31490cd37",
   "metadata": {},
   "source": [
    "Now that we have our LLM attribution object, we can similarly call `.attribute()` to obtain our gradient-based attributions. Right now, `LLMGradientAttribution` can only handle `TextTokenInput` inputs. We can visualize the attribution with respect to both the full output sequence and individual output tokens using the methods `.plot_seq_attr()` and `.plot_token_attr()`, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9121ab1b-8102-4aa9-9dc8-bd28b9c0144c",
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = TextTokenInput(\n",
    "    eval_prompt,\n",
    "    tokenizer,\n",
    "    skip_tokens=skip_tokens,\n",
    ")\n",
    "\n",
    "attr_res = llm_attr.attribute(inp, target=target, skip_tokens=skip_tokens)\n",
    "\n",
    "attr_res.plot_seq_attr(show=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c2d579e-4c40-491c-b1b9-2b5e7d284d0f",
   "metadata": {},
   "source": [
    "Layer Integrated Gradients estimates that the most important input token in the prediction of the subsequent tokens in the sentence is the word, \"lives.\" We can visualize further token-level attribution at the embedding layer as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "788d8ad3-b546-47af-943d-0b4d82f353d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "attr_res.plot_token_attr(show=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "952c3264-b91b-425b-946a-0989385303ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f388ff5e-ac5c-4391-9f9a-375df969cf4e",
   "metadata": {},
   "source": [
    "Keep in mind that the token- and sequence-wise attribution will change layer to layer. We encourage you to explore how this attribution changes with alternative layers in the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6588ec75-fb6a-4003-9919-225b9e847fc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Forward hook did not obtain any outputs for given layer",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 44\u001b[0m\n\u001b[0;32m     41\u001b[0m baseline_embeds \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros_like(layer(input_ids))\n\u001b[0;32m     43\u001b[0m \u001b[38;5;66;03m# Compute attributions\u001b[39;00m\n\u001b[1;32m---> 44\u001b[0m attributions, delta \u001b[38;5;241m=\u001b[39m layer_gradient_shap\u001b[38;5;241m.\u001b[39mattribute(\n\u001b[0;32m     45\u001b[0m     inputs\u001b[38;5;241m=\u001b[39mlayer(input_ids),        \u001b[38;5;66;03m# Embeddings of input tokens\u001b[39;00m\n\u001b[0;32m     46\u001b[0m     baselines\u001b[38;5;241m=\u001b[39mbaseline_embeds,     \u001b[38;5;66;03m# Baseline embeddings\u001b[39;00m\n\u001b[0;32m     47\u001b[0m     target\u001b[38;5;241m=\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39mconvert_tokens_to_ids(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m),  \u001b[38;5;66;03m# Target token for attribution\u001b[39;00m\n\u001b[0;32m     48\u001b[0m     return_convergence_delta\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m     49\u001b[0m )\n\u001b[0;32m     51\u001b[0m \u001b[38;5;66;03m# Aggregate attributions for visualization\u001b[39;00m\n\u001b[0;32m     52\u001b[0m attributions_sum \u001b[38;5;241m=\u001b[39m attributions\u001b[38;5;241m.\u001b[39msum(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[1;32m~\\captum\\captum\\log\\dummy_log.py:39\u001b[0m, in \u001b[0;36mlog_usage.<locals>._log_usage.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[0;32m     36\u001b[0m \u001b[38;5;66;03m# pyre-fixme[53]: Captured variable `func` is not annotated.\u001b[39;00m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;66;03m# pyre-fixme[3]: Return type must be annotated.\u001b[39;00m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any):\n\u001b[1;32m---> 39\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\captum\\captum\\attr\\_core\\layer\\layer_gradient_shap.py:319\u001b[0m, in \u001b[0;36mLayerGradientShap.attribute\u001b[1;34m(self, inputs, baselines, n_samples, stdevs, target, additional_forward_args, return_convergence_delta, attribute_to_layer_input)\u001b[0m\n\u001b[0;32m    310\u001b[0m input_min_baseline_x_grad \u001b[38;5;241m=\u001b[39m LayerInputBaselineXGradient(\n\u001b[0;32m    311\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward_func,\n\u001b[0;32m    312\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer,\n\u001b[0;32m    313\u001b[0m     device_ids\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice_ids,\n\u001b[0;32m    314\u001b[0m     multiply_by_inputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmultiplies_by_inputs,\n\u001b[0;32m    315\u001b[0m )\n\u001b[0;32m    317\u001b[0m nt \u001b[38;5;241m=\u001b[39m NoiseTunnel(input_min_baseline_x_grad)\n\u001b[1;32m--> 319\u001b[0m attributions \u001b[38;5;241m=\u001b[39m nt\u001b[38;5;241m.\u001b[39mattribute\u001b[38;5;241m.\u001b[39m__wrapped__(\n\u001b[0;32m    320\u001b[0m     nt,  \u001b[38;5;66;03m# self\u001b[39;00m\n\u001b[0;32m    321\u001b[0m     inputs,\n\u001b[0;32m    322\u001b[0m     nt_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msmoothgrad\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    323\u001b[0m     nt_samples\u001b[38;5;241m=\u001b[39mn_samples,\n\u001b[0;32m    324\u001b[0m     stdevs\u001b[38;5;241m=\u001b[39mstdevs,\n\u001b[0;32m    325\u001b[0m     draw_baseline_from_distrib\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    326\u001b[0m     baselines\u001b[38;5;241m=\u001b[39mbaselines,\n\u001b[0;32m    327\u001b[0m     target\u001b[38;5;241m=\u001b[39mtarget,\n\u001b[0;32m    328\u001b[0m     additional_forward_args\u001b[38;5;241m=\u001b[39madditional_forward_args,\n\u001b[0;32m    329\u001b[0m     return_convergence_delta\u001b[38;5;241m=\u001b[39mreturn_convergence_delta,\n\u001b[0;32m    330\u001b[0m     attribute_to_layer_input\u001b[38;5;241m=\u001b[39mattribute_to_layer_input,\n\u001b[0;32m    331\u001b[0m )\n\u001b[0;32m    333\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m attributions\n",
      "File \u001b[1;32m~\\captum\\captum\\attr\\_core\\noise_tunnel.py:220\u001b[0m, in \u001b[0;36mNoiseTunnel.attribute\u001b[1;34m(self, inputs, nt_type, nt_samples, nt_samples_batch_size, stdevs, draw_baseline_from_distrib, **kwargs)\u001b[0m\n\u001b[0;32m    212\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(nt_samples_partition):\n\u001b[0;32m    213\u001b[0m     inputs_with_noise \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_add_noise_to_inputs(\n\u001b[0;32m    214\u001b[0m         nt_samples_batch_size, inputs, stdevs\n\u001b[0;32m    215\u001b[0m     )\n\u001b[0;32m    216\u001b[0m     (\n\u001b[0;32m    217\u001b[0m         attributions_partial,\n\u001b[0;32m    218\u001b[0m         is_attrib_tuple,\n\u001b[0;32m    219\u001b[0m         delta_partial,\n\u001b[1;32m--> 220\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compute_partial_attribution(\n\u001b[0;32m    221\u001b[0m         inputs_with_noise,\n\u001b[0;32m    222\u001b[0m         kwargs_copy,\n\u001b[0;32m    223\u001b[0m         is_inputs_tuple,\n\u001b[0;32m    224\u001b[0m         return_convergence_delta,\n\u001b[0;32m    225\u001b[0m     )\n\u001b[0;32m    227\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(sum_attributions) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    228\u001b[0m         sum_attributions \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;01mNone\u001b[39;00m] \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(attributions_partial)\n",
      "File \u001b[1;32m~\\captum\\captum\\attr\\_core\\noise_tunnel.py:401\u001b[0m, in \u001b[0;36mNoiseTunnel._compute_partial_attribution\u001b[1;34m(self, inputs_with_noise_partition, kwargs_partition, is_inputs_tuple, return_convergence_delta)\u001b[0m\n\u001b[0;32m    397\u001b[0m attr_func \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattribution_method\u001b[38;5;241m.\u001b[39mattribute\n\u001b[0;32m    398\u001b[0m \u001b[38;5;66;03m# smoothgrad_Attr(x) = 1 / n * sum(Attr(x + N(0, sigma^2))\u001b[39;00m\n\u001b[0;32m    399\u001b[0m \u001b[38;5;66;03m# NOTE: using __wrapped__ such that it does not log the inner logs\u001b[39;00m\n\u001b[1;32m--> 401\u001b[0m attributions \u001b[38;5;241m=\u001b[39m attr_func\u001b[38;5;241m.\u001b[39m__wrapped__(  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[0;32m    402\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattribution_method,  \u001b[38;5;66;03m# self\u001b[39;00m\n\u001b[0;32m    403\u001b[0m     (\n\u001b[0;32m    404\u001b[0m         inputs_with_noise_partition\n\u001b[0;32m    405\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m is_inputs_tuple\n\u001b[0;32m    406\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m inputs_with_noise_partition[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    407\u001b[0m     ),\n\u001b[0;32m    408\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs_partition,\n\u001b[0;32m    409\u001b[0m )\n\u001b[0;32m    410\u001b[0m delta \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    412\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_delta_supported \u001b[38;5;129;01mand\u001b[39;00m return_convergence_delta:\n",
      "File \u001b[1;32m~\\captum\\captum\\attr\\_core\\layer\\layer_gradient_shap.py:437\u001b[0m, in \u001b[0;36mLayerInputBaselineXGradient.attribute\u001b[1;34m(self, inputs, baselines, target, additional_forward_args, return_convergence_delta, attribute_to_layer_input, grad_kwargs)\u001b[0m\n\u001b[0;32m    427\u001b[0m rand_coefficient \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(\n\u001b[0;32m    428\u001b[0m     np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39muniform(\u001b[38;5;241m0.0\u001b[39m, \u001b[38;5;241m1.0\u001b[39m, inputs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]),\n\u001b[0;32m    429\u001b[0m     device\u001b[38;5;241m=\u001b[39minputs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mdevice,\n\u001b[0;32m    430\u001b[0m     dtype\u001b[38;5;241m=\u001b[39minputs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mdtype,\n\u001b[0;32m    431\u001b[0m )\n\u001b[0;32m    433\u001b[0m input_baseline_scaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(\n\u001b[0;32m    434\u001b[0m     _scale_input(\u001b[38;5;28minput\u001b[39m, baseline, rand_coefficient)\n\u001b[0;32m    435\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m \u001b[38;5;28minput\u001b[39m, baseline \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(inputs, baselines)\n\u001b[0;32m    436\u001b[0m )\n\u001b[1;32m--> 437\u001b[0m grads, _ \u001b[38;5;241m=\u001b[39m compute_layer_gradients_and_eval(\n\u001b[0;32m    438\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward_func,\n\u001b[0;32m    439\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer,\n\u001b[0;32m    440\u001b[0m     input_baseline_scaled,\n\u001b[0;32m    441\u001b[0m     target,\n\u001b[0;32m    442\u001b[0m     additional_forward_args,\n\u001b[0;32m    443\u001b[0m     device_ids\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice_ids,\n\u001b[0;32m    444\u001b[0m     attribute_to_layer_input\u001b[38;5;241m=\u001b[39mattribute_to_layer_input,\n\u001b[0;32m    445\u001b[0m     grad_kwargs\u001b[38;5;241m=\u001b[39mgrad_kwargs,\n\u001b[0;32m    446\u001b[0m )\n\u001b[0;32m    448\u001b[0m attr_baselines \u001b[38;5;241m=\u001b[39m _forward_layer_eval(\n\u001b[0;32m    449\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward_func,\n\u001b[0;32m    450\u001b[0m     baselines,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    454\u001b[0m     attribute_to_layer_input\u001b[38;5;241m=\u001b[39mattribute_to_layer_input,\n\u001b[0;32m    455\u001b[0m )\n\u001b[0;32m    457\u001b[0m attr_inputs \u001b[38;5;241m=\u001b[39m _forward_layer_eval(\n\u001b[0;32m    458\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward_func,\n\u001b[0;32m    459\u001b[0m     inputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    463\u001b[0m     attribute_to_layer_input\u001b[38;5;241m=\u001b[39mattribute_to_layer_input,\n\u001b[0;32m    464\u001b[0m )\n",
      "File \u001b[1;32m~\\captum\\captum\\_utils\\gradient.py:670\u001b[0m, in \u001b[0;36mcompute_layer_gradients_and_eval\u001b[1;34m(forward_fn, layer, inputs, target_ind, additional_forward_args, gradient_neuron_selector, device_ids, attribute_to_layer_input, output_fn, grad_kwargs)\u001b[0m\n\u001b[0;32m    618\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    619\u001b[0m \u001b[38;5;124;03mComputes gradients of the output with respect to a given layer as well\u001b[39;00m\n\u001b[0;32m    620\u001b[0m \u001b[38;5;124;03mas the output evaluation of the layer for an arbitrary forward function\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    665\u001b[0m \u001b[38;5;124;03m        Target layer output for given input.\u001b[39;00m\n\u001b[0;32m    666\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    667\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m    668\u001b[0m     \u001b[38;5;66;03m# saved_layer is a dictionary mapping device to a tuple of\u001b[39;00m\n\u001b[0;32m    669\u001b[0m     \u001b[38;5;66;03m# layer evaluations on that device.\u001b[39;00m\n\u001b[1;32m--> 670\u001b[0m     saved_layer, output \u001b[38;5;241m=\u001b[39m _forward_layer_distributed_eval(\n\u001b[0;32m    671\u001b[0m         forward_fn,\n\u001b[0;32m    672\u001b[0m         inputs,\n\u001b[0;32m    673\u001b[0m         layer,\n\u001b[0;32m    674\u001b[0m         target_ind\u001b[38;5;241m=\u001b[39mtarget_ind,\n\u001b[0;32m    675\u001b[0m         additional_forward_args\u001b[38;5;241m=\u001b[39madditional_forward_args,\n\u001b[0;32m    676\u001b[0m         attribute_to_layer_input\u001b[38;5;241m=\u001b[39mattribute_to_layer_input,\n\u001b[0;32m    677\u001b[0m         forward_hook_with_return\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    678\u001b[0m         require_layer_grads\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    679\u001b[0m     )\n\u001b[0;32m    680\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m output[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mnumel() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m, (\n\u001b[0;32m    681\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTarget not provided when necessary, cannot\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    682\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m take gradient with respect to multiple outputs.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    683\u001b[0m     )\n\u001b[0;32m    685\u001b[0m     \u001b[38;5;66;03m# pyre-fixme[6]: For 2nd argument expected `Dict[Module, Dict[device,\u001b[39;00m\n\u001b[0;32m    686\u001b[0m     \u001b[38;5;66;03m#  typing.Tuple[Tensor, ...]]]` but got `Module`.\u001b[39;00m\n",
      "File \u001b[1;32m~\\captum\\captum\\_utils\\gradient.py:353\u001b[0m, in \u001b[0;36m_forward_layer_distributed_eval\u001b[1;34m(forward_fn, inputs, layer, target_ind, additional_forward_args, attribute_to_layer_input, forward_hook_with_return, require_layer_grads)\u001b[0m\n\u001b[0;32m    350\u001b[0m         hook\u001b[38;5;241m.\u001b[39mremove()\n\u001b[0;32m    352\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(saved_layer) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 353\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mForward hook did not obtain any outputs for given layer\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    355\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m forward_hook_with_return:\n\u001b[0;32m    356\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m saved_layer, output\n",
      "\u001b[1;31mAssertionError\u001b[0m: Forward hook did not obtain any outputs for given layer"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from captum.attr import LayerGradientShap, visualization\n",
    "\n",
    "# Load pre-trained Meta Llama model and tokenizer\n",
    "model_name = \"meta-llama/Llama-3.2-1B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "model.eval()\n",
    "\n",
    "# Sample input text\n",
    "text = \"The advancements in AI are fascinating and transformative.\"\n",
    "\n",
    "# Tokenize input\n",
    "inputs = tokenizer(\n",
    "    text,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "input_ids = inputs[\"input_ids\"]\n",
    "\n",
    "# Select the embedding layer for attributions\n",
    "layer = model.model.embed_tokens  # Llama's embedding layer\n",
    "\n",
    "# Define a custom forward function\n",
    "def forward_func(inputs_embeds, attention_mask=None):\n",
    "    \"\"\"\n",
    "    Custom forward function for Llama, returning the probability of generating\n",
    "    the next token based on embeddings.\n",
    "    \"\"\"\n",
    "    outputs = model(inputs_embeds=inputs_embeds, attention_mask=attention_mask)\n",
    "    # For causal LM, focus on the last token logits for attribution\n",
    "    logits = outputs.logits[:, -1, :]  # Last token predictions\n",
    "    probs = torch.softmax(logits, dim=-1)\n",
    "    return probs  # Return probabilities for all vocabulary tokens\n",
    "\n",
    "# Initialize LayerGradientShap\n",
    "layer_gradient_shap = LayerGradientShap(forward_func, layer)\n",
    "\n",
    "# Baseline: Zero embeddings for Llama\n",
    "baseline_embeds = torch.zeros_like(layer(input_ids))\n",
    "\n",
    "# Compute attributions\n",
    "attributions, delta = layer_gradient_shap.attribute(\n",
    "    inputs=layer(input_ids),        # Embeddings of input tokens\n",
    "    baselines=baseline_embeds,     # Baseline embeddings\n",
    "    target=tokenizer.convert_tokens_to_ids(\".\"),  # Target token for attribution\n",
    "    return_convergence_delta=True\n",
    ")\n",
    "\n",
    "# Aggregate attributions for visualization\n",
    "attributions_sum = attributions.sum(dim=-1).squeeze(0)\n",
    "\n",
    "# Map attributions back to tokens\n",
    "tokens = tokenizer.convert_ids_to_tokens(input_ids[0])\n",
    "attribution_scores = attributions_sum.tolist()\n",
    "\n",
    "# Visualization (optional)\n",
    "visualization.visualize_text([visualization.VisualizationDataRecord(\n",
    "    word_attributions=attribution_scores,\n",
    "    pred_prob=torch.softmax(model(**inputs).logits[:, -1, :], dim=-1).max().item(),\n",
    "    pred_class=tokens[-1],  # Predicted last token\n",
    "    true_class=\"N/A\",       # Causal LM may not have a true class\n",
    "    attr_class=tokens[-1],\n",
    "    attr_score=attributions_sum.sum().item(),\n",
    "    raw_input=text\n",
    ")])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77da2607-a9ea-4f7d-b33a-ce119567f777",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
