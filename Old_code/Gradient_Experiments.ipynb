{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d5168224-89b8-4087-8907-92b1638ae7b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('The output represents the result of this linear equation given the input as the 3 input numbers: \\n\\ninput = ( 87 , 23 , 83 ) ; output = 10943 \\ninput = ( 29 , 85 , 18 ) ; output = 5668 \\ninput = ( 28 , 82 , 93 ) ; output = 12049 \\ninput = ( 23 , 16 , 9 ) ; output = 2134 \\ninput = ( 68 , 27 , 95 ) ; output = 11508 \\ninput = ( 37 , 3 , 55 ) ; output = 6117 \\ninput = ( 16 , 87 , 77 ) ; output = 10446 \\ninput = ( 1 , 35 , 18 ) ; output = 2894 \\ninput = ( 10 , 33 , 57 ) ; output = 6510 \\ninput = ( 95 , 55 , 17 ) ; output = 6649 \\ninput = ( 32 , 45 , 29 ) ; output = 5244 \\ninput = ( 72 , 54 , 85 ) ; output = ',\n",
       " 11769)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#task input Creation:\n",
    "from transformers import AutoTokenizer\n",
    "import random\n",
    "random.seed(13)\n",
    "model_id = \"Local-Meta-Llama-3.2-1B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "def MakeRegressionTask(tokenizer,Context=False,max_examples_token_length=200):\n",
    "    td=''\n",
    "\n",
    "    weights=[random.randint(0, 100),random.randint(0, 100),random.randint(0, 100)]\n",
    "\n",
    "    while True:\n",
    "        inN=[random.randint(0, 100),random.randint(0, 100),random.randint(0, 100)]\n",
    "        res=weights[0]*inN[0]+weights[1]*inN[1]+weights[2]*inN[2]\n",
    "        td_new=td+'input = ( '+str(inN[0])+' , '+str(inN[1])+' , '+str(inN[2])+' ) ; output = '+str(res)+' \\n'\n",
    "        if tokenizer(td, return_tensors=\"pt\").input_ids.shape[1]>max_examples_token_length:\n",
    "            break\n",
    "        else:\n",
    "            td=td_new\n",
    "\n",
    "    inN=[random.randint(0, 100),random.randint(0, 100),random.randint(0, 100)]\n",
    "    res=weights[0]*inN[0]+weights[1]*inN[1]+weights[2]*inN[2]\n",
    "    td=td+'input = ( '+str(inN[0])+' , '+str(inN[1])+' , '+str(inN[2])+' ) ; output = '\n",
    "    if Context:\n",
    "        td='The output represents the result of this linear equation given the input as the 3 input numbers: \\n\\n'+td\n",
    "    return td,res\n",
    "MakeRegressionTask(tokenizer,Context=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2255d66e-d504-48a0-a52c-ce85bf868a98",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e6dca800-cff8-4f53-ad99-4ef37fec476d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('The following words are classified by the sentiment they imply: \\n\\ninput = stringently ; output = negativ \\ninput = plush ; output = positiv \\ninput = compliant ; output = positiv \\ninput = restructure ; output = positiv \\ninput = titillating ; output = positiv \\ninput = delicacy ; output = positiv \\ninput = laudable ; output = positiv \\ninput = harboring ; output = negativ \\ninput = dexterous ; output = positiv \\ninput = wrest ; output = negativ \\ninput = mocks ; output = negativ \\ninput = courageousness ; output = positiv \\ninput = dawn ; output = positiv \\ninput = unsuccessful ; output = negativ \\ninput = gimmick ; output = negativ \\ninput = cheesy ; output = negativ \\ninput = politeness ; output = positiv \\ninput = rip-off ; output = negativ \\ninput = positive ; output = positiv \\ninput = traumatized ; output = negativ \\ninput = detriment ; output = negativ \\ninput = exceeds ; output = ',\n",
       " 'positiv')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f=open('./Datasets/positive-words.txt',\"r\")\n",
    "pos_words=f.read().split('\\n')[:-1]\n",
    "f.close()\n",
    "f=open('./Datasets/negative-words.txt',\"r\",encoding=\"ISO-8859-1\")\n",
    "neg_words=f.read().split('\\n')[:-1]\n",
    "f.close()\n",
    "\n",
    "def MakeClassificationTask(tokenizer,Context=False,max_examples_token_length=200):\n",
    "    td=''\n",
    "    \n",
    "    while True:\n",
    "\n",
    "        if random.randint(0, 1)==1:\n",
    "            td_new=td+'input = '+pos_words[random.randint(0, len(pos_words)-1)]+' ; output = positiv \\n'\n",
    "        else:\n",
    "            td_new=td+'input = '+neg_words[random.randint(0, len(neg_words)-1)]+' ; output = negativ \\n'\n",
    "        if tokenizer(td, return_tensors=\"pt\").input_ids.shape[1]>max_examples_token_length:\n",
    "            break\n",
    "        else:\n",
    "            td=td_new\n",
    "\n",
    "    res=None\n",
    "    if random.randint(0, 1)==1:\n",
    "        td=td+'input = '+pos_words[random.randint(0, len(pos_words)-1)]+' ; output = '\n",
    "        res='positiv'\n",
    "    else:\n",
    "        td=td+'input = '+neg_words[random.randint(0, len(neg_words)-1)]+' ; output = '\n",
    "        res='negativ'\n",
    "    if Context:\n",
    "        td='The following words are classified by the sentiment they imply: \\n\\n'+td\n",
    "    return td,res\n",
    "\n",
    "MakeClassificationTask(tokenizer,Context=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ba07e758-8d51-405b-8316-ef07a6484ccc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cl2/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:2068: UserWarning: for model.embed_tokens.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/cl2/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:2068: UserWarning: for model.layers.0.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/cl2/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:2068: UserWarning: for model.layers.0.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/cl2/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:2068: UserWarning: for model.layers.0.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/cl2/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:2068: UserWarning: for model.layers.0.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/cl2/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:2068: UserWarning: for model.layers.0.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/cl2/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:2068: UserWarning: for model.layers.0.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/cl2/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:2068: UserWarning: for model.layers.0.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/cl2/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:2068: UserWarning: for model.layers.0.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/cl2/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:2068: UserWarning: for model.layers.0.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/cl2/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:2068: UserWarning: for model.layers.1.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/cl2/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:2068: UserWarning: for model.layers.1.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/cl2/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:2068: UserWarning: for model.layers.1.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/cl2/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:2068: UserWarning: for model.layers.1.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/cl2/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:2068: UserWarning: for model.layers.1.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/cl2/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:2068: UserWarning: for model.layers.1.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/cl2/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:2068: UserWarning: for model.layers.1.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/cl2/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:2068: UserWarning: for model.layers.1.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/cl2/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:2068: UserWarning: for model.layers.1.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/cl2/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:2068: UserWarning: for model.layers.2.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/cl2/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:2068: UserWarning: for model.layers.2.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/cl2/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:2068: UserWarning: for model.layers.2.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/cl2/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:2068: UserWarning: for model.layers.2.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/cl2/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:2068: UserWarning: for model.layers.2.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/cl2/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:2068: UserWarning: for model.layers.2.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/cl2/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:2068: UserWarning: for model.layers.2.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/cl2/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:2068: UserWarning: for model.layers.2.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/cl2/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:2068: UserWarning: for model.layers.2.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/cl2/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:2068: UserWarning: for model.layers.3.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/cl2/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:2068: UserWarning: for model.layers.3.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/cl2/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:2068: UserWarning: for model.layers.3.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/cl2/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:2068: UserWarning: for model.layers.3.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/cl2/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:2068: UserWarning: for model.layers.3.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/cl2/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:2068: UserWarning: for model.layers.3.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/cl2/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:2068: UserWarning: for model.layers.3.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/cl2/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:2068: UserWarning: for model.layers.3.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/cl2/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:2068: UserWarning: for model.layers.3.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/cl2/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:2068: UserWarning: for model.layers.4.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/cl2/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:2068: UserWarning: for model.layers.4.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/cl2/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:2068: UserWarning: for model.layers.4.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/cl2/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:2068: UserWarning: for model.layers.4.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/cl2/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:2068: UserWarning: for model.layers.4.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/cl2/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:2068: UserWarning: for model.layers.4.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/cl2/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:2068: UserWarning: for model.layers.4.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/cl2/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:2068: UserWarning: for model.layers.4.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/cl2/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:2068: UserWarning: for model.layers.4.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/cl2/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:2068: UserWarning: for model.layers.5.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/cl2/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:2068: UserWarning: for model.layers.5.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/cl2/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:2068: UserWarning: for model.layers.5.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/cl2/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:2068: UserWarning: for model.layers.5.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/cl2/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:2068: UserWarning: for model.layers.5.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/cl2/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:2068: UserWarning: for model.layers.5.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/cl2/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:2068: UserWarning: for model.layers.5.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/cl2/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:2068: UserWarning: for model.layers.5.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/cl2/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:2068: UserWarning: for model.layers.5.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/cl2/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:2068: UserWarning: for model.layers.6.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/cl2/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:2068: UserWarning: for model.layers.6.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/cl2/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:2068: UserWarning: for model.layers.6.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/cl2/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:2068: UserWarning: for model.layers.6.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/cl2/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:2068: UserWarning: for model.layers.6.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/cl2/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:2068: UserWarning: for model.layers.6.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/cl2/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:2068: UserWarning: for model.layers.6.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/cl2/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:2068: UserWarning: for model.layers.6.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/cl2/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:2068: UserWarning: for model.layers.6.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/cl2/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:2068: UserWarning: for model.layers.7.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/cl2/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:2068: UserWarning: for model.layers.7.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/cl2/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:2068: UserWarning: for model.layers.7.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/cl2/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:2068: UserWarning: for model.layers.7.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/cl2/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:2068: UserWarning: for model.layers.7.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/cl2/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:2068: UserWarning: for model.layers.7.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/cl2/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:2068: UserWarning: for model.layers.7.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/cl2/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:2068: UserWarning: for model.layers.7.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/cl2/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:2068: UserWarning: for model.layers.7.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/cl2/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:2068: UserWarning: for model.layers.8.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/cl2/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:2068: UserWarning: for model.layers.8.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/cl2/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:2068: UserWarning: for model.layers.8.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/cl2/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:2068: UserWarning: for model.layers.8.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/cl2/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:2068: UserWarning: for model.layers.8.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/cl2/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:2068: UserWarning: for model.layers.8.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/cl2/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:2068: UserWarning: for model.layers.8.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/cl2/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:2068: UserWarning: for model.layers.8.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/cl2/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:2068: UserWarning: for model.layers.8.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/cl2/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:2068: UserWarning: for model.layers.9.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/cl2/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:2068: UserWarning: for model.layers.9.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/cl2/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:2068: UserWarning: for model.layers.9.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/cl2/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:2068: UserWarning: for model.layers.9.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/cl2/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:2068: UserWarning: for model.layers.9.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/cl2/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:2068: UserWarning: for model.layers.9.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/cl2/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:2068: UserWarning: for model.layers.9.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/cl2/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:2068: UserWarning: for model.layers.9.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/cl2/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:2068: UserWarning: for model.layers.9.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/cl2/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:2068: UserWarning: for model.layers.10.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/cl2/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:2068: UserWarning: for model.layers.10.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/cl2/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:2068: UserWarning: for model.layers.10.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/cl2/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:2068: UserWarning: for model.layers.10.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/cl2/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:2068: UserWarning: for model.layers.10.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/cl2/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:2068: UserWarning: for model.layers.10.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/cl2/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:2068: UserWarning: for model.layers.10.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/cl2/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:2068: UserWarning: for model.layers.10.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/cl2/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:2068: UserWarning: for model.layers.10.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/cl2/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:2068: UserWarning: for model.layers.11.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/cl2/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:2068: UserWarning: for model.layers.11.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/cl2/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:2068: UserWarning: for model.layers.11.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/cl2/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:2068: UserWarning: for model.layers.11.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/cl2/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:2068: UserWarning: for model.layers.11.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/cl2/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:2068: UserWarning: for model.layers.11.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/cl2/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:2068: UserWarning: for model.layers.11.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/cl2/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:2068: UserWarning: for model.layers.11.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/cl2/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:2068: UserWarning: for model.layers.11.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/cl2/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:2068: UserWarning: for model.layers.12.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/cl2/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:2068: UserWarning: for model.layers.12.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/cl2/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:2068: UserWarning: for model.layers.12.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/cl2/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:2068: UserWarning: for model.layers.12.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/cl2/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:2068: UserWarning: for model.layers.12.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/cl2/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:2068: UserWarning: for model.layers.12.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/cl2/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:2068: UserWarning: for model.layers.12.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/cl2/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:2068: UserWarning: for model.layers.12.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/cl2/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:2068: UserWarning: for model.layers.12.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/cl2/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:2068: UserWarning: for model.layers.13.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/cl2/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:2068: UserWarning: for model.layers.13.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/cl2/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:2068: UserWarning: for model.layers.13.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/cl2/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:2068: UserWarning: for model.layers.13.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/cl2/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:2068: UserWarning: for model.layers.13.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/cl2/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:2068: UserWarning: for model.layers.13.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/cl2/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:2068: UserWarning: for model.layers.13.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/cl2/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:2068: UserWarning: for model.layers.13.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/cl2/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:2068: UserWarning: for model.layers.13.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/cl2/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:2068: UserWarning: for model.layers.14.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/cl2/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:2068: UserWarning: for model.layers.14.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/cl2/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:2068: UserWarning: for model.layers.14.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/cl2/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:2068: UserWarning: for model.layers.14.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/cl2/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:2068: UserWarning: for model.layers.14.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/cl2/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:2068: UserWarning: for model.layers.14.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/cl2/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:2068: UserWarning: for model.layers.14.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/cl2/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:2068: UserWarning: for model.layers.14.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/cl2/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:2068: UserWarning: for model.layers.14.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/cl2/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:2068: UserWarning: for model.layers.15.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/cl2/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:2068: UserWarning: for model.layers.15.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/cl2/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:2068: UserWarning: for model.layers.15.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/cl2/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:2068: UserWarning: for model.layers.15.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/cl2/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:2068: UserWarning: for model.layers.15.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/cl2/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:2068: UserWarning: for model.layers.15.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/cl2/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:2068: UserWarning: for model.layers.15.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/cl2/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:2068: UserWarning: for model.layers.15.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/cl2/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:2068: UserWarning: for model.layers.15.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/home/cl2/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:2068: UserWarning: for model.norm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "The model weights are not tied. Please use the `tie_weights` method before using the `infer_auto_device` function.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "from accelerate import infer_auto_device_map, init_empty_weights\n",
    "from torch.utils.checkpoint import checkpoint\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "model_id = \"Local-Meta-Llama-3.2-1B\"\n",
    "with init_empty_weights():\n",
    "    my_model = AutoModelForCausalLM.from_pretrained(model_id)\n",
    "device_map = infer_auto_device_map(my_model, max_memory={0: \"1GiB\", \"cpu\": \"30GiB\"})\n",
    "\n",
    "# Load the model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id,device_map=device_map)\n",
    "model.gradient_checkpointing_enable()\n",
    "\n",
    "\n",
    "#def zero_out_grad_hook(grad):\n",
    "#    return torch.zeros_like(grad) \n",
    "#for param in model.parameters():\n",
    "#    param.register_hook(lambda grad: None)\n",
    "\n",
    "def move_grad_to_cpu_hook(grad):\n",
    "    return grad.cpu()  # Move the gradient to CPU (RAM)\n",
    "\n",
    "# Register the hook for all parameters in the model\n",
    "#for param in model.parameters():\n",
    "#     param.requires_grad = False\n",
    "for param in model.lm_head.parameters():  # Unfreeze the output layer\n",
    "    param.requires_grad = True\n",
    "\n",
    "def move_grad_to_cpu(grad):\n",
    "    return grad.to('cpu')  # Move the gradient to CPU\n",
    "\n",
    "# Register the hook on each parameter\n",
    "for param in model.parameters():\n",
    "    param.register_hook(move_grad_to_cpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "71d3d8fe-6129-4c6a-a0ff-c82f0be2e808",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(128256, 2048)\n",
       "    (layers): ModuleList(\n",
       "      (0-15): 16 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaSdpaAttention(\n",
       "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (k_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "          (v_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "          (up_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "          (down_proj): Linear(in_features=8192, out_features=2048, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2048, out_features=128256, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "758fe848-5da0-4589-ac9b-65a7c066de61",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.model.layers[2].self_attn.k_proj\n",
    "layers_to_hook = {}\n",
    "layers_to_hook[\"embed_tokens\"]=[model.model.embed_tokens]\n",
    "for i in model.model.layers:\n",
    "    \n",
    "    if \"q_proj\" not in layers_to_hook:\n",
    "        layers_to_hook[\"q_proj\"]=[]\n",
    "    layers_to_hook[\"q_proj\"].append(i.self_attn.q_proj)\n",
    "    \n",
    "    if \"k_proj\" not in layers_to_hook:\n",
    "        layers_to_hook[\"k_proj\"]=[]\n",
    "    layers_to_hook[\"k_proj\"].append(i.self_attn.k_proj)\n",
    "    \n",
    "    if \"v_proj\" not in layers_to_hook:\n",
    "        layers_to_hook[\"v_proj\"]=[]\n",
    "    layers_to_hook[\"v_proj\"].append(i.self_attn.v_proj)\n",
    "    \n",
    "    if \"o_proj\" not in layers_to_hook:\n",
    "        layers_to_hook[\"o_proj\"]=[]\n",
    "    layers_to_hook[\"o_proj\"].append(i.self_attn.o_proj)\n",
    "    \n",
    "    if \"rotary_emb\" not in layers_to_hook:\n",
    "        layers_to_hook[\"rotary_emb\"]=[]\n",
    "    layers_to_hook[\"rotary_emb\"].append(i.self_attn.rotary_emb)\n",
    "    \n",
    "    if \"mlp\" not in layers_to_hook:\n",
    "        layers_to_hook[\"mlp\"]=[]\n",
    "    layers_to_hook[\"mlp\"].append(i.mlp)\n",
    "\n",
    "\n",
    "layers_to_hook[\"embed_tokens\"]=[model.model.rotary_emb]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "51f6219c-c989-4645-a64f-845b4a27219b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary to store the output of each hooked layer and its gradient\n",
    "extracted_outputs = {}\n",
    "\n",
    "def move_to_cpu_with_grad(data):\n",
    "    if isinstance(data, torch.Tensor):  # Check if it's a tensor\n",
    "        # Move to CPU, ensure requires_grad is True, and retain gradients\n",
    "        data = data.to('cpu').requires_grad_(True)\n",
    "        data.retain_grad()  # Retain gradients for non-leaf tensors\n",
    "        return data\n",
    "    elif isinstance(data, dict):  # If it's a dictionary, recursively check its values\n",
    "        return {key: move_to_cpu_with_grad(value) for key, value in data.items()}\n",
    "    elif isinstance(data, list):  # If it's a list, recursively check each element\n",
    "        return [move_to_cpu_with_grad(item) for item in data]\n",
    "    elif isinstance(data, tuple):  # If it's a tuple, recursively check each element\n",
    "        return tuple(move_to_cpu_with_grad(item) for item in data)\n",
    "    else:\n",
    "        return data  # If it's not a tensor, return it as-is\n",
    "\n",
    "# Hook function factory that returns a hook function for each layer\n",
    "def create_hook_fn(layer_name,layer_index):\n",
    "    def hook_fn(module, input, output):\n",
    "        if layer_name not in extracted_outputs:\n",
    "            extracted_outputs[layer_name] = {}\n",
    "        extracted_outputs[layer_name][layer_index]=move_to_cpu_with_grad(output)\n",
    "    return hook_fn\n",
    "\n",
    "# Example: Hooking multiple layers (k_proj and v_proj in the first transformer layer)\n",
    "# You can add more layers similarly by changing the names and layers.\n",
    "\n",
    "# Register hooks for the desired layers\n",
    "hooks = []\n",
    "for layer_name, layer_arr in layers_to_hook.items():\n",
    "    for layer_pos,layer in enumerate(layer_arr):\n",
    "        hook = layer.register_forward_hook(create_hook_fn(layer_name,layer_pos))\n",
    "        hooks.append(hook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e5c33e5f-233a-4392-b225-63bb05ab77e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_177892/2876764701.py:5: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n"
     ]
    }
   ],
   "source": [
    "text = \"This is a test sentence\"\n",
    "tokenized_output = tokenizer(text, return_tensors=\"pt\")\n",
    "tokenized_output = {key: value.to('cpu') for key, value in tokenized_output.items()}\n",
    "with torch.autograd.graph.save_on_cpu():\n",
    "    with autocast():\n",
    "        M_outputs = model(**tokenized_output)  # Perform the forward pass without storing parameter gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6a300b42-d9d8-4b89-905e-9ff027213762",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['embed_tokens', 'q_proj', 'k_proj', 'v_proj', 'o_proj', 'mlp'])\n",
      "tensor(7.5657, grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(extracted_outputs.keys())\n",
    "print(M_outputs.logits[0][-1][398])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "38093370-ad93-4ba7-a05d-a07e1e385d74",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_177892/501562521.py:4: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  GradScaler().scale(loss).backward()\n"
     ]
    }
   ],
   "source": [
    "loss = M_outputs.logits[0][-1][23987] # Use the k_proj output for backward pass\n",
    "model.zero_grad()\n",
    "#loss.backward()\n",
    "GradScaler().scale(loss).backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b0864ac3-40fb-4bb5-abfa-171ea3338a94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 2.2605e+01,  2.6987e+01,  2.9781e+01, -1.3537e+02,  4.2770e+01,\n",
       "          -1.1903e+01, -1.3949e+01, -5.1854e+02, -7.2073e+02, -1.3281e+02,\n",
       "           1.8215e+02, -1.6404e+03,  2.5903e+01, -9.7003e+02, -6.5150e+02,\n",
       "          -5.8118e+02, -4.9534e+03, -9.1092e+03, -5.3643e+03, -1.8437e+03,\n",
       "          -3.1328e+03, -8.3125e+03, -2.0272e+03, -1.6877e+04, -1.0086e+04,\n",
       "          -2.9641e+03, -8.5136e+03,  7.0061e+03, -4.7000e+03, -5.9806e+03,\n",
       "          -1.5357e+04, -5.1297e+04, -1.0618e+01, -4.7002e+01, -1.5457e+02,\n",
       "          -6.9218e+01,  5.0785e+01, -9.5006e+01, -7.7247e+01,  1.9884e+02,\n",
       "          -2.6148e+02,  1.2428e+02, -8.4040e+02, -2.4184e+03,  1.6465e+02,\n",
       "          -1.5038e+03, -2.6871e+03, -9.2840e+02, -3.6808e+02, -6.8971e+02,\n",
       "          -3.3897e+03, -7.2401e+03, -2.1939e+04, -6.7925e+03, -9.1170e+03,\n",
       "          -5.8670e+03, -5.1162e+03,  1.3867e+04,  4.4069e+03, -1.5600e+03,\n",
       "          -1.3662e+03, -1.5843e+04, -4.6871e+03,  8.2495e+03],\n",
       "         [-4.2099e+03,  3.7992e+03,  2.1746e+03,  1.7567e+03,  4.6565e+03,\n",
       "           2.9389e+03,  5.1802e+01,  5.3441e+03,  2.6360e+03,  2.6284e+03,\n",
       "          -5.4920e+03, -4.9860e+03, -8.5924e+03,  4.2404e+03,  1.3414e+03,\n",
       "           4.4474e+03,  2.2965e+03,  1.4247e+02, -3.8968e+02,  9.7107e+02,\n",
       "           3.1742e+03,  2.6828e+03,  2.8813e+03, -3.0143e+03,  1.9766e+03,\n",
       "          -7.7122e+02, -3.4226e+03,  2.0799e+03, -3.9157e+03, -6.8663e+03,\n",
       "          -6.3021e+03, -1.0292e+03,  1.9203e+03,  3.6411e+03,  6.3141e+03,\n",
       "           6.4122e+02,  8.9896e+03,  1.7621e+02,  3.0835e+03,  4.3487e+03,\n",
       "           5.9929e+03,  1.5348e+03,  1.0230e+04,  5.1037e+03,  9.7138e+03,\n",
       "           9.3614e+03,  6.1492e+03,  1.2850e+03,  8.9043e+02,  8.3110e+03,\n",
       "          -2.7641e+03,  5.0306e+03,  2.2137e+03, -1.1698e+03, -3.5242e+03,\n",
       "          -6.0572e+03, -1.8928e+03,  8.3919e+03, -7.8884e+02, -2.0255e+03,\n",
       "          -6.0505e+03, -9.0724e+03, -5.5259e+03, -5.2887e+02],\n",
       "         [ 4.7928e+03,  2.2930e+03, -2.6656e+03,  4.6248e+01,  1.8182e+03,\n",
       "           1.5991e+02,  1.7189e+03, -1.5956e+02,  3.1834e+03,  1.3425e+03,\n",
       "           2.4480e+03,  1.4512e+03,  2.2971e+03, -5.9985e+02,  2.2860e+03,\n",
       "           1.3675e+03, -1.0419e+03, -3.2931e+03,  1.2101e+03, -1.9365e+02,\n",
       "          -1.7733e+04, -7.6266e+03, -4.7225e+03,  1.3378e+03,  3.1110e+03,\n",
       "          -5.8319e+03,  4.3894e+03,  7.4877e+03, -1.6907e+04, -2.7088e+03,\n",
       "          -1.4515e+03, -8.2350e+03, -8.6911e+03, -6.6933e+03, -9.4561e+02,\n",
       "           7.1198e+02,  3.1963e+03,  2.0280e+03,  2.6274e+03,  2.9600e+03,\n",
       "          -1.6621e+02,  5.0975e+03,  4.0946e+03,  1.6687e+03,  4.1310e+03,\n",
       "          -1.5038e+02, -6.4817e+02, -9.5850e+02, -1.6379e+03, -3.9223e+03,\n",
       "           2.0097e+02,  3.2240e+03, -7.9338e+03, -4.3831e+03, -5.5223e+03,\n",
       "          -7.0104e+02, -6.8961e+02, -2.0041e+03, -1.0863e+03, -1.0373e+04,\n",
       "           5.2620e+02, -1.9799e+03, -4.7938e+03, -7.1352e+03],\n",
       "         [ 2.0257e+03,  2.0298e+03,  9.0152e+02, -1.9046e+02,  3.0440e+03,\n",
       "           4.2335e+03,  3.1056e+03, -2.2710e+03,  5.6090e+03, -8.8771e+02,\n",
       "           8.0534e+02,  2.7052e+03,  5.5082e+03,  4.0381e+03,  6.1044e+02,\n",
       "           3.0889e+03,  1.1304e+03,  4.7931e+02,  2.2340e+03,  1.2188e+02,\n",
       "           3.4043e+03,  1.2390e+04, -1.4673e+04, -3.4163e+03,  2.4004e+03,\n",
       "          -1.3643e+04, -2.1521e+03, -4.9277e+02, -1.3688e+04, -3.7837e+03,\n",
       "           3.8069e+02, -1.6641e+03, -2.6662e+02,  2.4120e+03,  8.5561e+03,\n",
       "           6.4297e+03,  9.8951e+03,  3.4728e+03,  6.6948e+03,  1.0243e+04,\n",
       "           1.0064e+03,  1.1686e+04,  7.3640e+03,  1.0785e+03,  6.1616e+03,\n",
       "           1.3118e+04, -1.2799e+03,  4.1910e+03, -3.4174e+03, -5.5782e+02,\n",
       "           1.6136e+02,  9.0686e+02, -6.9453e+03,  1.3965e+04, -1.6496e+04,\n",
       "          -4.2296e+03,  2.0777e+03, -1.1796e+04,  9.2773e+03, -1.1811e+04,\n",
       "          -5.0600e+03,  2.0792e+03, -6.6023e+03, -5.6021e+03],\n",
       "         [-7.8003e+03, -2.4871e+04,  1.7532e+03,  2.5490e+04,  7.9007e+03,\n",
       "           6.2267e+03,  6.9130e+03,  3.3045e+04,  2.2357e+04,  2.9392e+04,\n",
       "           1.3868e+04,  1.3255e+04,  2.8478e+03,  6.2239e+03,  1.7075e+04,\n",
       "           1.9781e+03,  1.0153e+03, -1.0001e+04, -5.1256e+03,  1.0284e+04,\n",
       "          -5.2221e+04, -5.6787e+04, -2.2664e+04, -6.8704e+03, -3.6108e+04,\n",
       "          -1.9684e+04, -1.0750e+04, -1.5086e+04,  1.4381e+03, -6.2163e+03,\n",
       "          -1.2291e+04, -6.7298e+04, -3.3117e+04, -5.5980e+03, -1.2693e+03,\n",
       "           1.0395e+04,  2.8118e+04,  2.2941e+04,  1.2033e+04,  6.4938e+03,\n",
       "           1.3910e+04,  2.4440e+04,  1.5420e+04,  2.2113e+04,  1.5828e+04,\n",
       "           9.7738e+03,  2.2536e+04,  7.0516e+03,  2.1454e+04, -8.0770e+01,\n",
       "          -1.9374e+03, -1.1424e+03, -1.1366e+05, -5.5300e+04, -1.0674e+04,\n",
       "          -1.0704e+04, -1.6800e+04, -1.0350e+04,  1.6509e+04,  4.3323e+03,\n",
       "          -3.6490e+04,  1.9597e+03,  3.4914e+04, -1.6823e+04],\n",
       "         [-4.5801e+04, -2.9536e+04, -2.8525e+04, -8.4989e+03, -1.2383e+04,\n",
       "           1.4036e+04, -2.0148e+04,  1.3558e+04,  4.4453e+04,  4.4824e+04,\n",
       "           2.5443e+03, -3.1192e+03, -2.3824e+04, -9.1209e+03,  1.4533e+04,\n",
       "          -2.6231e+03, -5.3727e+03, -2.7627e+04,  4.7073e+03,  7.8869e+03,\n",
       "          -7.6162e+04,  1.7592e+04, -3.1151e+04,  8.8365e+03, -4.4124e+04,\n",
       "           6.1048e+04, -1.8483e+04, -7.7397e+03,  2.2903e+04, -3.1870e+04,\n",
       "          -3.3002e+04, -9.8909e+04,  6.0273e+04,  5.9390e+03,  1.5077e+04,\n",
       "          -6.9890e+03, -2.0848e+03, -1.5784e+04,  1.1236e+04,  5.1614e+03,\n",
       "          -1.3666e+04, -7.2956e+03,  3.7024e+02,  6.2347e+03, -1.6569e+04,\n",
       "           8.0194e+03,  2.1524e+04,  2.4077e+04, -6.6601e+03, -3.2534e+03,\n",
       "          -1.1961e+04,  1.1595e+04, -1.7423e+05,  1.2726e+04, -3.5394e+04,\n",
       "          -4.5721e+01, -3.0379e+04,  6.7906e+04,  7.5374e+04, -9.0738e+03,\n",
       "          -5.7609e+03, -3.4341e+04,  4.1957e+04,  6.0071e+04]]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extracted_outputs[\"embed_tokens\"][0][0].grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d45843c-5204-4c58-9a34-d0277a810be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Experiments until here "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e7c3277-43dc-4438-9423-716e8088a1f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access gradients for the hooked layers\n",
    "k_proj_output_grad = extracted_outputs[\"k_proj_output\"].grad\n",
    "v_proj_output_grad = extracted_outputs[\"v_proj_output\"].grad\n",
    "\n",
    "# Remove the hooks (optional)\n",
    "for hook in hooks:\n",
    "    hook.remove()\n",
    "\n",
    "# Print gradients for the hooked layers\n",
    "print(\"Gradient of k_proj output:\", k_proj_output_grad)\n",
    "print(\"Gradient of v_proj output:\", v_proj_output_grad)rotary_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37f5493e-d7c7-43b3-97d7-c6474221f7d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Find the k_proj layer in the attention mechanism (first transformer layer here)\n",
    "layer = model.encoder.layer[0].attention.self.key  # Key projection layer (k_proj)\n",
    "\n",
    "# Register the forward hook\n",
    "hook = layer.register_forward_hook(hook_fn)\n",
    "\n",
    "# Tokenize the input\n",
    "text = \"This is a test sentence.\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "\n",
    "# Perform forward pass\n",
    "outputs = model(**inputs)\n",
    "\n",
    "# Now, the output of k_proj has been captured and retain_grad() called\n",
    "k_proj_output = extracted_outputs[\"k_proj_output\"]\n",
    "\n",
    "# We can create a scalar loss or sum of the final output to perform backward on\n",
    "# For simplicity, let's sum the outputs of the last hidden state (you can replace this with any loss function)\n",
    "loss = outputs.last_hidden_state.sum()\n",
    "\n",
    "# Perform the backward pass to compute gradients\n",
    "loss.backward()\n",
    "\n",
    "# Access the gradient of the k_proj output after backward pass\n",
    "k_proj_output_grad = k_proj_output.grad\n",
    "\n",
    "# Remove the hook (optional)\n",
    "hook.remove()\n",
    "\n",
    "# Print the gradient for k_proj output\n",
    "print(\"Gradient of k_proj output:\", k_proj_output_grad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "adec7bef-a3cd-4924-ab88-d5c8a714c76a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient of k_proj output: tensor([[[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         ...,\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.]]])\n",
      "{'k_proj_output': tensor([[[ 1.2797e+00,  2.2044e-01,  2.4081e-01,  ..., -1.7203e-02,\n",
      "          -2.6863e-01,  7.2731e-01],\n",
      "         [ 3.5969e-01, -4.7975e-01, -9.1391e-02,  ..., -8.5932e-01,\n",
      "           2.7313e-01, -2.7672e+00],\n",
      "         [ 5.3447e-01, -5.9713e-01,  2.5200e-03,  ..., -1.1064e+00,\n",
      "          -3.9094e-01, -3.2550e+00],\n",
      "         ...,\n",
      "         [-3.1752e-01, -6.9256e-01, -3.3283e-01,  ..., -1.9510e+00,\n",
      "          -1.2154e-01, -2.9600e+00],\n",
      "         [ 1.0200e-02,  1.6724e-01, -8.6441e-02,  ...,  2.2651e+00,\n",
      "          -1.1164e+00, -3.7025e+00],\n",
      "         [-6.6954e-01,  1.0858e+00,  6.5653e-02,  ..., -6.1973e-02,\n",
      "          -1.1029e+00, -2.5208e+00]]], grad_fn=<ViewBackward0>), 'v_proj_output': tensor([[[ 2.3786, -0.0979, -0.2844,  ..., -0.0202,  0.1028,  0.1020],\n",
      "         [ 0.0421,  0.2387, -0.3388,  ...,  0.0105,  0.4314, -0.2452],\n",
      "         [-1.0016,  0.4564,  0.1488,  ..., -0.1093,  0.0487, -0.1741],\n",
      "         ...,\n",
      "         [-0.1331,  0.2453, -0.3614,  ...,  0.3154,  0.2008,  0.4104],\n",
      "         [ 0.0911,  0.4011,  0.1578,  ..., -0.2568, -0.4099,  0.2087],\n",
      "         [ 0.6495,  0.0901,  0.2915,  ...,  0.1073,  0.1519,  0.0450]]],\n",
      "       grad_fn=<ViewBackward0>)}\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModel, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "# Load the model and tokenizer\n",
    "model_name = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "for param in model.parameters():\n",
    "   param.requires_grad = False  # Disable gradient tracking\n",
    "\n",
    "# Dictionary to store the output of k_proj and its gradient\n",
    "extracted_outputs = {}\n",
    "\n",
    "\n",
    "\n",
    "# Hook function to capture the output of the k_proj layer\n",
    "def create_hook_fn(layer_name):\n",
    "    def hook_fn(module, input, output):\n",
    "        extracted_outputs[layer_name] = output.to(device=\"cpu\")\n",
    "        output.requires_grad_(True)\n",
    "        output.retain_grad()  # Retain gradients for this layer's output only\n",
    "    return hook_fn\n",
    "\n",
    "# Example: Hooking multiple layers (k_proj and v_proj in the first transformer layer)\n",
    "# You can add more layers similarly by changing the names and layers.\n",
    "layers_to_hook = {\n",
    "    \"k_proj_output\": model.encoder.layer[0].attention.self.key,  # Key projection layer\n",
    "    \"v_proj_output\": model.encoder.layer[0].attention.self.value  # Value projection layer\n",
    "}\n",
    "\n",
    "# Tokenize the input\n",
    "text = \"This is a test sentence.\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "\n",
    "\n",
    "# Find the k_proj layer in the attention mechanism (first transformer layer here)\n",
    "layer = model.encoder.layer[0].attention.self.key  # Key projection layer (k_proj)\n",
    "\n",
    "hooks = []\n",
    "for layer_name, layer in layers_to_hook.items():\n",
    "    hook = layer.register_forward_hook(create_hook_fn(layer_name))\n",
    "    hooks.append(hook)\n",
    "\n",
    "outputs = model(**inputs)  # Perform the forward pass without storing parameter gradients\n",
    "\n",
    "\n",
    "\n",
    "# Re-enable gradient computation for intermediate layers\n",
    "k_proj_output = extracted_outputs[\"v_proj_output\"]\n",
    "\n",
    "# Enable gradient computation for k_proj output only\n",
    "k_proj_output.requires_grad_(True)  # Ensure gradients are enabled for the k_proj output\n",
    "\n",
    "# Create a scalar loss or sum of the final output to perform backward on\n",
    "loss = k_proj_output.sum()  # Use k_proj output directly for backward pass\n",
    "\n",
    "# Perform the backward pass to compute gradients\n",
    "loss.backward()\n",
    "\n",
    "# Access the gradient of the k_proj output after the backward pass\n",
    "k_proj_output_grad = k_proj_output.grad\n",
    "\n",
    "# Remove the hook (optional)\n",
    "hook.remove()\n",
    "\n",
    "# Print the gradient for k_proj output\n",
    "print(\"Gradient of k_proj output:\", k_proj_output_grad)\n",
    "print(extracted_outputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b05961c4-0d92-40ce-9d9a-f26f5dc69457",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91bd6ea5-c508-4dc1-8818-6a1cc51f956a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "# Load the model and tokenizer\n",
    "model_name = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "# Dictionary to store the output of each hooked layer and its gradient\n",
    "extracted_outputs = {}\n",
    "\n",
    "# Hook function factory that returns a hook function for each layer\n",
    "def create_hook_fn(layer_name):\n",
    "    def hook_fn(module, input, output):\n",
    "        extracted_outputs[layer_name] = output\n",
    "        output.retain_grad()  # Retain gradients for this layer's output only\n",
    "    return hook_fn\n",
    "\n",
    "# Example: Hooking multiple layers (k_proj and v_proj in the first transformer layer)\n",
    "# You can add more layers similarly by changing the names and layers.\n",
    "layers_to_hook = {\n",
    "    \"k_proj_output\": model.encoder.layer[0].attention.self.key,  # Key projection layer\n",
    "    \"v_proj_output\": model.encoder.layer[0].attention.self.value  # Value projection layer\n",
    "}\n",
    "\n",
    "# Register hooks for the desired layers\n",
    "hooks = []\n",
    "for layer_name, layer in layers_to_hook.items():\n",
    "    hook = layer.register_forward_hook(create_hook_fn(layer_name))\n",
    "    hooks.append(hook)\n",
    "\n",
    "# Tokenize the input\n",
    "text = \"This is a test sentence.\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "\n",
    "# Perform the forward pass (with hooks capturing intermediate outputs)\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)  # Forward pass without storing gradients for parameters\n",
    "\n",
    "# Re-enable gradient computation for the extracted outputs\n",
    "for layer_name, output in extracted_outputs.items():\n",
    "    output.requires_grad_(True)  # Enable gradient computation for the extracted outputs\n",
    "\n",
    "# Example: Use the output of \"k_proj_output\" for backward pass\n",
    "k_proj_output = extracted_outputs[\"k_proj_output\"]\n",
    "loss = k_proj_output.sum()  # Use the k_proj output for backward pass\n",
    "loss.backward()\n",
    "\n",
    "# Access gradients for the hooked layers\n",
    "k_proj_output_grad = extracted_outputs[\"k_proj_output\"].grad\n",
    "v_proj_output_grad = extracted_outputs[\"v_proj_output\"].grad\n",
    "\n",
    "# Remove the hooks (optional)\n",
    "for hook in hooks:\n",
    "    hook.remove()\n",
    "\n",
    "# Print gradients for the hooked layers\n",
    "print(\"Gradient of k_proj output:\", k_proj_output_grad)\n",
    "print(\"Gradient of v_proj output:\", v_proj_output_grad)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
