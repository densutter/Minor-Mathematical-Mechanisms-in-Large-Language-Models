{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "682e0d6c-d9ab-4112-9bff-9c80736e8222",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hey\n",
      "Generated feature matrix X:\n",
      " [[12  9 11 10]\n",
      " [ 1  7  4 11]\n",
      " [ 2 11 11 17]\n",
      " [ 3  8 18 10]\n",
      " [ 3  3  5  9]]\n",
      "Generated labels y:\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2]\n",
      "True logistic regression weights (W):\n",
      " [[ 4  7  7 17]\n",
      " [11 17  4  1]\n",
      " [ 5  2  1 18]]\n",
      "Class distribution: {0: 34, 1: 33, 2: 33}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def generate_balanced_multiclass_logistic_data(n_samples, n_classes, d_dimensions, max_deviation=1):\n",
    "    \"\"\"\n",
    "    Generates a balanced dataset for a multiclass logistic regression problem, perfectly classified by integer weights,\n",
    "    with both features and weights as positive integers. Each class is equally represented in the dataset, and labels\n",
    "    are assigned based on the weight matrix W.\n",
    "    \n",
    "    Parameters:\n",
    "    - n_samples: int, total number of samples to generate\n",
    "    - n_classes: int, number of classes\n",
    "    - d_dimensions: int, number of dimensions\n",
    "    - max_deviation: int, maximum allowed deviation for near-uniqueness in feature space\n",
    "    \n",
    "    Returns:\n",
    "    - X: np.array, shape (n_samples, d_dimensions), the feature matrix with positive integer coordinates\n",
    "    - y: np.array, shape (n_samples,), the labels\n",
    "    - W: np.array, shape (n_classes, d_dimensions), the integer weights used to generate data\n",
    "    \"\"\"\n",
    "    #np.random.seed(0)  # For reproducibility\n",
    "    \n",
    "    # Generate random integer weights for each class, in range [0, 18]\n",
    "    allowed_Tries=n_samples*100\n",
    "    samples_per_class = n_samples // n_classes\n",
    "    extra_samples = n_samples % n_classes  # Any remainder will add one extra sample to some classes\n",
    "    \n",
    "    while True:\n",
    "        actual_Tries=0\n",
    "        print(\"hey\")\n",
    "        W = np.random.randint(0, 19, size=(n_classes, d_dimensions))\n",
    "        \n",
    "        # Calculate the number of samples per class\n",
    "    \n",
    "        X_list = []\n",
    "        y_list = []\n",
    "    \n",
    "        for class_idx in range(n_classes):\n",
    "            if actual_Tries>allowed_Tries:\n",
    "                break\n",
    "            class_samples = []\n",
    "            class_labels = []\n",
    "    \n",
    "            # Generate enough samples for this class\n",
    "            n_class_samples = samples_per_class + (1 if class_idx < extra_samples else 0)\n",
    "            \n",
    "            while len(class_samples) < n_class_samples:                \n",
    "                if actual_Tries>allowed_Tries:\n",
    "                    break\n",
    "                actual_Tries+=1\n",
    "                #print(actual_Tries,allowed_Tries)\n",
    "                # Generate a random feature vector with integer values\n",
    "                X_sample = np.random.randint(1, 20, size=(1, d_dimensions))\n",
    "                \n",
    "                # Compute the logits for this sample using the weight matrix W\n",
    "                logits = X_sample @ W.T  # Shape: (1, n_classes)\n",
    "                \n",
    "                # Assign the label based on the class with the highest logit\n",
    "                predicted_class = np.argmax(logits)\n",
    "                \n",
    "                # If the predicted class matches the target class, keep the sample\n",
    "                if predicted_class == class_idx:\n",
    "                    # Perturb slightly to prevent ambiguity in weight solutions\n",
    "                    #X_sample += np.random.randint(-max_deviation, max_deviation + 1, size=(1, d_dimensions))\n",
    "                    #X_sample = np.clip(X_sample, 1, 100)  # Keep values positive\n",
    "    \n",
    "                    class_samples.append(X_sample.flatten())\n",
    "                    class_labels.append(class_idx)\n",
    "    \n",
    "            # Append to the main lists\n",
    "            X_list.extend(class_samples)\n",
    "            y_list.extend(class_labels)\n",
    "\n",
    "        if actual_Tries<=allowed_Tries:\n",
    "            # Convert to numpy arrays\n",
    "            X = np.array(X_list)\n",
    "            y = np.array(y_list)\n",
    "        \n",
    "            # Shuffle the dataset to avoid any class order patterns\n",
    "            #indices = np.arange(n_samples)\n",
    "            #np.random.shuffle(indices)\n",
    "            #X, y = X[indices], y[indices]\n",
    "        \n",
    "            return X, y, W\n",
    "\n",
    "# Example usage\n",
    "n_samples = 100\n",
    "n_classes = 3\n",
    "d_dimensions = 4\n",
    "max_deviation = 1  # Small scope for weight deviation\n",
    "\n",
    "X, y, true_weights = generate_balanced_multiclass_logistic_data(n_samples, n_classes, d_dimensions, max_deviation)\n",
    "\n",
    "print(\"Generated feature matrix X:\\n\", X[:5])  # Print first 5 samples for inspection\n",
    "print(\"Generated labels y:\\n\", y)          # Print first 5 labels for inspection\n",
    "print(\"True logistic regression weights (W):\\n\", true_weights)\n",
    "\n",
    "# Check class balance\n",
    "unique, counts = np.unique(y, return_counts=True)\n",
    "print(\"Class distribution:\", dict(zip(unique, counts)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9db05682-3d5f-4f28-a97e-7d20e89240cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hey\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('The output represents the result of this linear classification given 2 dimensions and 3 classes: \\n\\ninput = ( 0 , 16 ) ; output = 1 \\ninput = ( 14 , 13 ) ; output = 0 \\ninput = ( 13 , 17 ) ; output = 0 \\ninput = ( 2 , 6 ) ; output = 1 \\ninput = ( 2 , 0 ) ; output = 2 \\ninput = ( 7 , 2 ) ; output = 2 \\ninput = ( 15 , 17 ) ; output = 0 \\ninput = ( 10 , 9 ) ; output = 0 \\ninput = ( 1 , 16 ) ; output = 1 \\ninput = ( 6 , 1 ) ; output = 2 \\ninput = ( 11 , 2 ) ; output = 2 \\ninput = ( 13 , 2 ) ; output = 2 \\ninput = ( 7 , 2 ) ; output = 2 \\ninput = ( 2 , 6 ) ; output = 1 \\ninput = ( 8 , 0 ) ; output = ',\n",
       " '2',\n",
       " [array([[ 5, 12],\n",
       "         [ 2, 14],\n",
       "         [11,  0]]),\n",
       "  array([[192, 224,   0],\n",
       "         [226, 210, 154],\n",
       "         [269, 264, 143],\n",
       "         [ 82,  88,  22],\n",
       "         [ 10,   4,  22],\n",
       "         [ 59,  42,  77],\n",
       "         [279, 268, 165],\n",
       "         [158, 146, 110],\n",
       "         [197, 226,  11],\n",
       "         [ 42,  26,  66],\n",
       "         [ 79,  50, 121],\n",
       "         [ 89,  54, 143],\n",
       "         [ 59,  42,  77],\n",
       "         [ 82,  88,  22]])])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import LLM_Tasks\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_id = \"Local-Meta-Llama-3.2-1B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "print(\"hey\")\n",
    "ac_task=LLM_Tasks.Multiclass_Logistic_Regression_Task(tokenizer,Display_Context=True)\n",
    "ac_task.Generate_Task()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4f9475b1-c4fb-4910-8936-9a3bbe66d0e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.3794, -1.1828, -0.2481],\n",
      "         [-1.2955, -1.8468,  0.0469]]])\n",
      "tensor([[-1.4528, -0.6071, -1.0515, -0.9251],\n",
      "        [-0.0682,  0.6161,  0.8818, -0.7948],\n",
      "        [ 0.7523, -1.1920,  1.7286,  1.1223]])\n",
      "tensor([[[ 0.4453, -0.2027, -1.0730,  1.0127],\n",
      "         [ 2.0435, -0.4072, -0.1853,  2.7191]]])\n",
      "torch.Size([1, 4])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def attention_weighted_sum(values, attention, head_dim, output_dim=256):\n",
    "    \"\"\"\n",
    "    Function to project heads, apply softmax to attention, and compute weighted sum.\n",
    "    \n",
    "    Parameters:\n",
    "    values (torch.Tensor): Tensor of shape (tokens, heads * head_dim), representing values from different heads.\n",
    "    attention (torch.Tensor): Tensor of shape (tokens, heads), representing attention weights for each head.\n",
    "    head_dim (int): Dimension of each head before projection.\n",
    "    output_dim (int): Dimension to project each head to (default is 256).\n",
    "    \n",
    "    Returns:\n",
    "    torch.Tensor: Tensor of shape (tokens, output_dim) representing the weighted sum of projected heads.\n",
    "    \"\"\"\n",
    "    tokens, total_dim = values.shape\n",
    "    heads = total_dim // head_dim\n",
    "    \n",
    "    # Reshape values to separate heads\n",
    "    values = values.view(tokens, heads, head_dim)  # (tokens, heads, head_dim)\n",
    "\n",
    "    # Define the projection matrix, shared across heads, to project each head to output_dim\n",
    "    projection_matrix = torch.randn(head_dim, output_dim)  # Shape: (head_dim, output_dim)\n",
    "\n",
    "    # Project each head to output_dim\n",
    "    print(values)\n",
    "    print(projection_matrix)\n",
    "    projected_values = torch.einsum('thd,do->tho', values, projection_matrix)  # Shape: (tokens, heads, output_dim)\n",
    "    print(projected_values)\n",
    "    \n",
    "    # Apply softmax on the attention matrix such that all elements sum to 1\n",
    "    attention_weights = F.softmax(attention.view(-1), dim=0).view(tokens, heads)  # Shape: (tokens, heads)\n",
    "\n",
    "    # Use attention weights to perform weighted sum on the projected values\n",
    "    weighted_sum = torch.einsum('th,tho->to', attention_weights, projected_values)  # Shape: (tokens, output_dim)\n",
    "\n",
    "    return weighted_sum  # Shape: (tokens, output_dim)\n",
    "\n",
    "# Example usage\n",
    "tokens = 1\n",
    "heads = 2\n",
    "head_dim = 3\n",
    "values = torch.randn(tokens, heads * head_dim)\n",
    "attention = torch.randn(tokens, heads)\n",
    "\n",
    "output = attention_weighted_sum(values, attention, head_dim,output_dim=4)\n",
    "print(output.shape)  # Should print torch.Size([tokens, 256])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9d14687f-a5c5-4a14-9192-57b7e448cfdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 256])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CustomAttention(nn.Module):\n",
    "    def __init__(self, heads, head_dim, hidden_dim_project=256, hidden_dim_layer=256, output_dim=10, num_layers=1):\n",
    "        \"\"\"\n",
    "        Initialize the CustomAttention module.\n",
    "\n",
    "        Parameters:\n",
    "        heads (int): Number of attention heads.\n",
    "        head_dim (int): Dimension of each attention head.\n",
    "        output_dim (int): Dimension to project each head to (default is 256).\n",
    "        layer_config (str): Either \"linear\" for a single output layer or \"mlp\" for a multi-layer MLP.\n",
    "        num_layers (int): Number of layers for the output MLP (only applies if layer_config=\"mlp\").\n",
    "        \"\"\"\n",
    "        super(CustomAttention, self).__init__()\n",
    "        self.heads = heads\n",
    "        self.head_dim = head_dim\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "        # Learnable projection matrix (shared across heads)\n",
    "        self.projection_matrix = nn.Parameter(torch.randn(head_dim, hidden_dim_project))\n",
    "\n",
    "        # Learnable attention matrix\n",
    "        self.attention_matrix = nn.Parameter(torch.randn(heads))\n",
    "\n",
    "        # Define the output layers based on the layer_config\n",
    "        self.num_layers = num_layers\n",
    "        self.output_layers = nn.ModuleList()\n",
    "\n",
    "\n",
    "        last_dim=hidden_dim_project\n",
    "        for i in range(num_layers-1):\n",
    "            new_dim=hidden_dim_layer\n",
    "            self.output_layers.append(nn.Linear(last_dim, new_dim))\n",
    "            self.output_layers.append(nn.GELU())\n",
    "            last_dim=new_dim\n",
    "        self.output_layers.append(nn.Linear(last_dim, output_dim))\n",
    "\n",
    "    def forward(self, values):\n",
    "        \"\"\"\n",
    "        Forward pass through the CustomAttention module.\n",
    "\n",
    "        Parameters:\n",
    "        values (torch.Tensor): Input tensor of shape (tokens, heads * head_dim).\n",
    "        \n",
    "        Returns:\n",
    "        torch.Tensor: Output tensor of shape (tokens, output_dim).\n",
    "        \"\"\"\n",
    "        tokens, total_dim = values.shape\n",
    "        assert total_dim == self.heads * self.head_dim, \\\n",
    "            f\"Expected values to have shape (tokens, {self.heads * self.head_dim}), but got {values.shape}\"\n",
    "\n",
    "        # Reshape to (tokens, heads, head_dim) to separate heads\n",
    "        values = values.view(tokens, self.heads, self.head_dim)\n",
    "\n",
    "        # Project each head to output_dim using the learnable projection_matrix\n",
    "        projected_values = torch.einsum('thd,do->tho', values, self.projection_matrix)  # (tokens, heads, output_dim)\n",
    "\n",
    "        # Apply softmax on the learnable attention matrix (global normalization across tokens and heads)\n",
    "        attention_weights = F.softmax(self.attention_matrix, dim=0)  # Shape: (heads,)\n",
    "        \n",
    "        # Reshape attention weights to (1, heads) for broadcasting with projected_values\n",
    "        attention_weights = attention_weights.unsqueeze(0)  # Shape: (1, heads)\n",
    "        \n",
    "        # Use attention weights to perform weighted sum on the projected values\n",
    "        weighted_sum = torch.einsum('th,tho->to', attention_weights, projected_values)  # Shape: (tokens, output_dim)\n",
    "\n",
    "        # Apply the output layer(s) based on the layer configuration\n",
    "        output = weighted_sum\n",
    "        for layer in self.output_layers:\n",
    "            output = layer(output)\n",
    "\n",
    "        return output\n",
    "\n",
    "# Example usage\n",
    "tokens = 10\n",
    "heads = 8\n",
    "head_dim = 64\n",
    "output_dim = 256\n",
    "\n",
    "# Define input values tensor of shape (tokens, heads * head_dim)\n",
    "values = torch.randn(tokens, heads * head_dim)\n",
    "\n",
    "# Initialize the CustomAttention module with MLP output configuration\n",
    "custom_attention = CustomAttention(heads=heads, head_dim=head_dim, output_dim=output_dim, num_layers=2)\n",
    "\n",
    "# Forward pass\n",
    "output = custom_attention(values)\n",
    "print(output.shape)  # Should print torch.Size([tokens, 256])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "de38e90b-5d8a-4aa3-a51c-cf3b95bda96d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([256])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CustomAttention(nn.Module):\n",
    "    def __init__(self, heads, head_dim, hidden_dim_project=256, hidden_dim_layer=256, output_dim=10, num_layers=1):\n",
    "        \"\"\"\n",
    "        Initialize the CustomAttention module.\n",
    "\n",
    "        Parameters:\n",
    "        heads (int): Number of attention heads.\n",
    "        head_dim (int): Dimension of each attention head.\n",
    "        output_dim (int): Dimension to project each head to (default is 256).\n",
    "        num_layers (int): Number of layers for the output MLP (only applies if layer_config=\"mlp\").\n",
    "        \"\"\"\n",
    "        super(CustomAttention, self).__init__()\n",
    "        self.heads = heads\n",
    "        self.head_dim = head_dim\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "        # Learnable projection matrix (shared across heads)\n",
    "        self.projection_matrix = nn.Parameter(torch.randn(head_dim, hidden_dim_project))\n",
    "\n",
    "        # Learnable attention matrix\n",
    "        self.attention_matrix = nn.Parameter(torch.randn(heads))\n",
    "\n",
    "        # Define the output layers based on the layer_config\n",
    "        self.num_layers = num_layers\n",
    "        self.output_layers = nn.ModuleList()\n",
    "\n",
    "        last_dim = hidden_dim_project\n",
    "        for i in range(num_layers - 1):\n",
    "            new_dim = hidden_dim_layer\n",
    "            self.output_layers.append(nn.Linear(last_dim, new_dim))\n",
    "            self.output_layers.append(nn.GELU())\n",
    "            last_dim = new_dim\n",
    "        self.output_layers.append(nn.Linear(last_dim, output_dim))\n",
    "\n",
    "    def forward(self, values):\n",
    "        \"\"\"\n",
    "        Forward pass through the CustomAttention module.\n",
    "\n",
    "        Parameters:\n",
    "        values (torch.Tensor): Input tensor of shape (tokens, heads * head_dim).\n",
    "        \n",
    "        Returns:\n",
    "        torch.Tensor: Output tensor of shape (output_dim).\n",
    "        \"\"\"\n",
    "        tokens, total_dim = values.shape\n",
    "        assert total_dim == self.heads * self.head_dim, \\\n",
    "            f\"Expected values to have shape (tokens, {self.heads * self.head_dim}), but got {values.shape}\"\n",
    "\n",
    "        # Reshape to (tokens, heads, head_dim) to separate heads\n",
    "        values = values.view(tokens, self.heads, self.head_dim)\n",
    "\n",
    "        # Project each head to hidden_dim_project using the learnable projection_matrix\n",
    "        projected_values = torch.einsum('thd,do->tho', values, self.projection_matrix)  # Shape: (tokens, heads, hidden_dim_project)\n",
    "\n",
    "        # Apply softmax on the learnable attention matrix (global normalization across heads)\n",
    "        attention_weights = F.softmax(self.attention_matrix, dim=0)  # Shape: (heads,)\n",
    "        \n",
    "        # Reshape attention weights to (1, heads) for broadcasting with projected_values\n",
    "        attention_weights = attention_weights.unsqueeze(0)  # Shape: (1, heads)\n",
    "        \n",
    "        # Use attention weights to perform weighted sum on the projected values\n",
    "        weighted_sum = torch.einsum('th,tho->to', attention_weights, projected_values)  # Shape: (tokens, hidden_dim_project)\n",
    "\n",
    "        # Aggregate over the tokens by summing them up\n",
    "        aggregated_tokens = weighted_sum.sum(dim=0)  # Shape: (hidden_dim_project)\n",
    "\n",
    "        # Pass through the output layers\n",
    "        output = aggregated_tokens\n",
    "        for layer in self.output_layers:\n",
    "            output = layer(output)\n",
    "\n",
    "        return output\n",
    "\n",
    "# Example usage\n",
    "tokens = 10\n",
    "heads = 8\n",
    "head_dim = 64\n",
    "output_dim = 256\n",
    "\n",
    "# Define input values tensor of shape (tokens, heads * head_dim)\n",
    "values = torch.randn(tokens, heads * head_dim)\n",
    "\n",
    "# Initialize the CustomAttention module with MLP output configuration\n",
    "custom_attention = CustomAttention(heads=heads, head_dim=head_dim, output_dim=output_dim, num_layers=2)\n",
    "\n",
    "# Forward pass\n",
    "output = custom_attention(values)\n",
    "print(output.shape)  # Should print torch.Size([256])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "55e79cfb-2b79-488a-9a6c-a3f1543144e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First extract relevance map "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "58064b7a-6eb8-43cd-b261-e6243d4102e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then test if the ones with the highest difference are representative of the respective \n",
    "\n",
    "\n",
    "# 3 step strategy:\n",
    "## test out with the one in the paper how much of it is saved in that layer :)\n",
    "## Test out how the performance is if we only look at the most important vs the least important for one task perform (given the difference mapping I get of the feature importance map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "55487b2f-184e-40ca-895d-4a7a38fe7edf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CustomAttention(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, max_tokens, projection_dim=512, hidden_dim=512, num_layers=1):\n",
    "        \"\"\"\n",
    "        Initialize the CustomAttention module.\n",
    "\n",
    "        Parameters:\n",
    "        output_dim (int): Dimension of the input tensor per token.\n",
    "        projection_dim (int): Dimension to project each token to (default is 512).\n",
    "        max_tokens (int): Maximum number of tokens expected in the input.\n",
    "        num_layers (int): Number of layers for the output MLP.\n",
    "        \"\"\"\n",
    "        super(CustomAttention, self).__init__()\n",
    "\n",
    "        self.input_dim=input_dim\n",
    "        self.max_tokens=max_tokens\n",
    "        # Learnable initial projection layer (output_dim -> projection_dim)\n",
    "        self.initial_projection = nn.Linear(input_dim, projection_dim)\n",
    "\n",
    "        # Learnable attention vector of length max_tokens\n",
    "        self.attention_vector = nn.Parameter(torch.randn(max_tokens))\n",
    "\n",
    "        # Output MLP layers\n",
    "        self.output_layers = nn.ModuleList()\n",
    "        last_dim = projection_dim\n",
    "        for _ in range(num_layers - 1):\n",
    "            new_dim=hidden_dim\n",
    "            self.output_layers.append(nn.Linear(last_dim, new_dim))\n",
    "            self.output_layers.append(nn.GELU())\n",
    "            last_dim = new_dim\n",
    "        self.output_layers.append(nn.Linear(last_dim, output_dim))\n",
    "\n",
    "    def forward(self, values):\n",
    "        \"\"\"\n",
    "        Forward pass through the CustomAttention module.\n",
    "\n",
    "        Parameters:\n",
    "        values (torch.Tensor): Input tensor of shape (tokens, output_dim).\n",
    "        \n",
    "        Returns:\n",
    "        torch.Tensor: Output tensor of shape (projection_dim).\n",
    "        \"\"\"\n",
    "        tokens, in_dim = values.shape\n",
    "        assert in_dim == self.input_dim, \\\n",
    "            f\"Expected values to have shape (tokens, {self.output_dim}), but got {values.shape}\"\n",
    "        assert tokens <= self.max_tokens, \\\n",
    "            f\"Number of tokens ({tokens}) exceeds max_tokens ({self.max_tokens})\"\n",
    "\n",
    "        # Project each token from output_dim to projection_dim\n",
    "        values = self.initial_projection(values)  # Shape: (tokens, projection_dim)\n",
    "\n",
    "        # Apply softmax on the learnable attention vector (up to `tokens` entries)\n",
    "        attention_weights = F.softmax(self.attention_vector[:tokens], dim=0)  # Shape: (tokens,)\n",
    "\n",
    "        # Perform weighted sum across tokens\n",
    "        weighted_sum = (attention_weights.unsqueeze(1) * values).sum(dim=0)  # Shape: (projection_dim)\n",
    "        \n",
    "        # Pass through the output layers\n",
    "        output = weighted_sum\n",
    "        for layer in self.output_layers:\n",
    "            output = layer(output)\n",
    "\n",
    "        return output\n",
    "\n",
    "# Example usage\n",
    "tokens = 10\n",
    "input_dim = 256\n",
    "projection_dim = 512\n",
    "max_tokens = 10\n",
    "\n",
    "# Define input values tensor of shape (tokens, output_dim)\n",
    "values = torch.randn(tokens, input_dim)\n",
    "\n",
    "# Initialize the CustomAttention module\n",
    "custom_attention = CustomAttention(input_dim=input_dim,output_dim=10,max_tokens=100, projection_dim=projection_dim, num_layers=2)\n",
    "\n",
    "# Forward pass\n",
    "output = custom_attention(values)\n",
    "print(output.shape)  # Should print torch.Size([512])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d648df0d-c89b-491d-8287-c192faf7be00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 4 2 1 3]\n",
      "Top values:\n",
      " [[20 40]\n",
      " [25 45]\n",
      " [22 42]]\n",
      "Bottom values:\n",
      " [[10 50]\n",
      " [15 55]\n",
      " [12 52]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def selective_keep(Input_Arr, Masking_Arr, keep):\n",
    "    # Get the number of dimensions\n",
    "    input_dim = Masking_Arr.shape[0]\n",
    "    \n",
    "    # Calculate the number of top/bottom indices to keep\n",
    "    keep_count = int(np.ceil(keep * input_dim))\n",
    "\n",
    "    # Get the indices of dimensions with highest and lowest values in Masking_Arr\n",
    "    sorted_indices = np.argsort(Masking_Arr)  # Sort Masking_Arr to find the min and max indices\n",
    "    print(sorted_indices)\n",
    "    top_indices = sorted_indices[-keep_count:]  # Top 'keep_count' indices (highest Masking_Arr values)\n",
    "    bottom_indices = sorted_indices[:keep_count]  # Bottom 'keep_count' indices (lowest Masking_Arr values)\n",
    "\n",
    "    # Select the top and bottom values from Input_Arr for each token\n",
    "    top_values = Input_Arr[:, top_indices]\n",
    "    bottom_values = Input_Arr[:, bottom_indices]\n",
    "\n",
    "    return top_values, bottom_values\n",
    "\n",
    "\n",
    "# Example input\n",
    "Masking_Arr = np.array([0.2, 0.8, 0.5, 0.9, 0.3])\n",
    "Input_Arr = np.array([\n",
    "    [10, 20, 30, 40, 50],\n",
    "    [15, 25, 35, 45, 55],\n",
    "    [12, 22, 32, 42, 52]\n",
    "])\n",
    "keep = 0.4  # Keep 40% of dimensions\n",
    "\n",
    "# Call the function\n",
    "top_values, bottom_values = selective_keep(Input_Arr, Masking_Arr, keep)\n",
    "\n",
    "print(\"Top values:\\n\", top_values)\n",
    "print(\"Bottom values:\\n\", bottom_values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "15e8e98e-4341-45af-8b69-e39f83962eb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The multidimensional lists are the same\n"
     ]
    }
   ],
   "source": [
    "list1 = [[1, 2, 3], [4, 5, 6]]\n",
    "list2 = [[1, 2, 3], [4, 5, 6]]\n",
    "\n",
    "if list1 == list2:\n",
    "    print(\"The multidimensional lists are the same\")\n",
    "else:\n",
    "    print(\"The multidimensional lists are different\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
